{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b82edf33",
   "metadata": {},
   "source": [
    "# Deep Learning Course Project\n",
    "## Phase 2\n",
    "### Saba Nasiri - 98101052\n",
    "### Aliasghar Pourghani - 98101299\n",
    "### Dariush Ghaemi - 98109678"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9055fc83",
   "metadata": {},
   "source": [
    "# tf-idf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "98daadc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "from sklearn import svm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b2ae2d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fc479b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "580c1036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_remover(sent):\n",
    "    filtered_sentence = []\n",
    "    sent = sent.split(' ')\n",
    "    for w in sent:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    \n",
    "    return ' '.join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "82948b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer_train = TfidfVectorizer()\n",
    "vectorizer_test = TfidfVectorizer()\n",
    "\n",
    "\n",
    "# Reading input data\n",
    "\n",
    "english_train_add = \"./MSCTD_dataset/english_train.txt\"\n",
    "english_test_add = \"./MSCTD_dataset/english_test.txt\"\n",
    "\n",
    "sentiment_train_add = \"./MSCTD_dataset/sentiment_train.txt\"\n",
    "sentiment_test_add = \"./MSCTD_dataset/sentiment_test.txt\"\n",
    "\n",
    "corpus_text_train = []\n",
    "corpus_text_test = []\n",
    "\n",
    "corpus_sentiment_train = []\n",
    "corpus_sentiment_test = []\n",
    "\n",
    "#####################################################################################\n",
    "txt_file = open(english_train_add, encoding=\"utf8\")\n",
    "\n",
    "for line in txt_file:\n",
    "    \n",
    "    a = re.sub(r'[^\\w\\s]','', stop_word_remover(line.strip())).lower()\n",
    "    corpus_text_train.append(a)\n",
    "\n",
    "train_tfidf = vectorizer_train.fit_transform(corpus_text_train)\n",
    "\n",
    "txt_file.close()\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "txt_file = open(english_test_add, encoding=\"utf8\")\n",
    "\n",
    "for line in txt_file:\n",
    "    \n",
    "    a = stop_word_remover(re.sub(r'[^\\w\\s]','', line.strip()).lower())\n",
    "    corpus_text_test.append(a)\n",
    "    \n",
    "\n",
    "test_tfidf = vectorizer_train.transform(corpus_text_test)\n",
    "\n",
    "txt_file.close()\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "txt_file = open(sentiment_train_add, encoding=\"utf8\")\n",
    "\n",
    "for line in txt_file:\n",
    "    corpus_sentiment_train.append(line.strip())\n",
    "\n",
    "txt_file.close()\n",
    "\n",
    "    \n",
    "    \n",
    "#####################################################################################\n",
    "txt_file = open(sentiment_test_add, encoding=\"utf8\")\n",
    "\n",
    "for line in txt_file:\n",
    "    corpus_sentiment_test.append(line.strip())\n",
    "    \n",
    "txt_file.close()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ef07ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dictionary = vectorizer_train.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "11d4c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf_np = test_tfidf.toarray()\n",
    "train_tfidf_np = train_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3f6425c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingSet_tfidf(Dataset):    \n",
    "    def __init__(self, line_vector, sentiment, transform=None, target_transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.sentiment = torch.tensor(sentiment)\n",
    "        self.line_vector = line_vector\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentiment)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        self.X = torch.tensor(self.line_vector[idx])\n",
    "        self.y = self.sentiment[idx]\n",
    "        return self.X, self.y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f36a5148",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sentiment_train2 = [int(corpus_sentiment_train[i]) for i in range(len(corpus_sentiment_train))]\n",
    "corpus_sentiment_test2 = [int(corpus_sentiment_test[i]) for i in range(len(corpus_sentiment_test))]\n",
    "transform = transforms.ToTensor()\n",
    "target_transform = transforms.ToTensor()\n",
    "train_dataset1 = EmbeddingSet_tfidf(train_tfidf_np, corpus_sentiment_train2, transform, target_transform)\n",
    "test_dataset1 = EmbeddingSet_tfidf(test_tfidf_np, corpus_sentiment_test2, transform, target_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bec28f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5067\n",
      "633\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset1,batch_size = batch_size,shuffle = True)\n",
    "batch_size2=1\n",
    "test_loader = DataLoader(test_dataset1,batch_size = batch_size2,shuffle = False)\n",
    "print(len(test_loader))\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8a308af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining network\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, inputsize):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputsize, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 512)\n",
    "        self.fc3 = nn.Linear(512, 128)\n",
    "        self.fc4 = nn.Linear(128, 32)\n",
    "        self.fc5 = nn.Linear(32, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.fc1(input)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x) \n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x) \n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x) \n",
    "        x = self.relu(x)\n",
    "        x = self.fc5(x) \n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e0d77902",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPnetwork = MLP(len(train_tfidf_np[0])).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f7258eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(MLPnetwork.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8b5c80d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(X.float())\n",
    "\n",
    "        loss = loss_fn(pred,y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "       \n",
    "    \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>11f}  [{current:>5d}/{size:>5d}]\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d6444bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X=X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred,y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    print(f\"Test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4ec4f2e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "epoch 0\n",
      "loss:    1.132343  [    0/20240]\n",
      "loss:    1.090298  [ 3200/20240]\n",
      "loss:    1.068661  [ 6400/20240]\n",
      "loss:    1.081574  [ 9600/20240]\n",
      "loss:    0.996265  [12800/20240]\n",
      "loss:    1.021556  [16000/20240]\n",
      "loss:    1.159712  [19200/20240]\n",
      "Test: \n",
      " Accuracy: 48.9%, Avg loss: 1.029445 \n",
      "\n",
      "======================================================\n",
      "epoch 1\n",
      "loss:    0.934937  [    0/20240]\n",
      "loss:    0.811899  [ 3200/20240]\n",
      "loss:    0.963568  [ 6400/20240]\n",
      "loss:    1.006706  [ 9600/20240]\n",
      "loss:    0.872145  [12800/20240]\n",
      "loss:    0.889208  [16000/20240]\n",
      "loss:    0.802548  [19200/20240]\n",
      "Test: \n",
      " Accuracy: 49.0%, Avg loss: 1.049467 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print('======================================================')\n",
    "    print('epoch '+str(i))\n",
    "    \n",
    "    train_loop(train_loader, MLPnetwork, criterion, optimizer)\n",
    "    test_loop(test_loader, MLPnetwork, criterion)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3302e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(MLPnetwork,\"tfidfModel.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b395cd",
   "metadata": {},
   "source": [
    "TF-IDF stands for term frequency-inverse document frequency and it is a measure, used in the fields of information retrieval (IR) and machine learning, that can quantify the importance or relevance of string representations (words, phrases, lemmas, etc)  in a document amongst a collection of documents (also known as a corpus).\n",
    "\n",
    "TF-IDF can be broken down into two parts TF (term frequency) and IDF (inverse document frequency).\n",
    "\n",
    "TF : \n",
    "\n",
    "Term frequency works by looking at the frequency of a particular term you are concerned with relative to the document. \n",
    "\n",
    "IDF : \n",
    "Inverse document frequency looks at how common (or uncommon) a word is amongst the corpus.\n",
    "\n",
    "In this part of the projact, tf-idf matrix of each corpus is used to embed sententences into vrctors. each row of the matrix is related to one line of the text. This row is used as the input of the network, and a mlp network is trained based on the labels of each sentence.\n",
    "\n",
    "The final accuracy of this part is 50.5%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951cc011",
   "metadata": {},
   "source": [
    "# tf-idf and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5d2a8ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 20240/20240 [37:17<00:00,  9.05it/s]\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "\n",
    "from tqdm import tqdm\n",
    "words_tfidf_vectors = {}\n",
    "g=-1\n",
    "for line in tqdm(corpus_text_train):\n",
    "    wordss = line.split(' ')\n",
    "    words = [wordss[j] for j in range(len(wordss)) if len(wordss[j])>1]\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        positive_label_words = words\n",
    "        k = len(positive_label_words)\n",
    "        \n",
    "        negative_label_words = random.choices(train_dictionary, k = k)\n",
    "        \n",
    "        X = []\n",
    "        \n",
    "        y_pos = [1]*k\n",
    "        y_neg = [0]*k\n",
    "        y = y_pos + y_neg\n",
    "        for w in positive_label_words + negative_label_words:\n",
    "            X.append(train_tfidf[:,np.where(train_dictionary == w)[0][0]].toarray().reshape(20240))\n",
    "        clf = svm.SVC()\n",
    "        clf.fit(X, y)\n",
    "        vectors = clf.support_vectors_[i]\n",
    "        vectors = np.float32(vectors)\n",
    "        if words_tfidf_vectors.get(word) is not None:\n",
    "            words_tfidf_vectors[word].append(vectors)\n",
    "        else:\n",
    "            words_tfidf_vectors[word]=[]\n",
    "            words_tfidf_vectors[word].append(vectors)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1db029b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeding_dictionary = {}\n",
    "\n",
    "for word in train_dictionary:\n",
    "    embeding_dictionary[word] = sum(words_tfidf_vectors[word])/len(words_tfidf_vectors[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "da614fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingSet(Dataset):    \n",
    "    def __init__(self, embedded_dictionary, text, sentiment, transform=None, target_transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.sentiment = torch.tensor(sentiment)\n",
    "        self.text = text\n",
    "        self.embedded_dictionary = embedded_dictionary\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentiment)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        self.X = torch.tensor([])\n",
    "        wordss = self.text[idx].split(' ')\n",
    "        words = [wordss[j] for j in range(len(wordss)) if len(wordss[j])>1]\n",
    "        word = [words[j] for j in range(len(words)) if words[j] in train_dictionary]\n",
    "        n = len(word)\n",
    "        for w in word:\n",
    "            self.X = torch.cat((self.X, self.transform(self.embedded_dictionary[w].reshape(1,-1))), 0)\n",
    "\n",
    "        self.X = self.X.squeeze(1).sum(0)/n\n",
    "        \n",
    "        self.y = self.sentiment[idx]\n",
    "\n",
    "        return self.X, self.y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "13b0a37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "target_transform = transforms.ToTensor()\n",
    "corpus_sentiment_train = [int(x) for x in corpus_sentiment_train]\n",
    "corpus_text_train2 = []\n",
    "corpus_sentiment_train2 = []\n",
    "for i in range(len(corpus_text_train)):\n",
    "    wordss = corpus_text_train[i].split(' ')\n",
    "    words = [wordss[j] for j in range(len(wordss)) if len(wordss[j])>1]\n",
    "    word = [words[j] for j in range(len(words)) if words[j] in train_dictionary]\n",
    "    n = len(word)\n",
    "    if n >0:\n",
    "        corpus_text_train2.append(corpus_text_train[i])\n",
    "        corpus_sentiment_train2.append(corpus_sentiment_train[i])\n",
    "        \n",
    "corpus_sentiment_test = [int(x) for x in corpus_sentiment_test]       \n",
    "corpus_text_test2 = []\n",
    "corpus_sentiment_test2 = []\n",
    "for i in range(len(corpus_text_test)):\n",
    "    wordss = corpus_text_test[i].split(' ')\n",
    "    words = [wordss[j] for j in range(len(wordss)) if len(wordss[j])>1]\n",
    "    word = [words[j] for j in range(len(words)) if words[j] in train_dictionary]\n",
    "    n = len(word)\n",
    "    if n >0:\n",
    "        corpus_text_test2.append(corpus_text_test[i])\n",
    "        corpus_sentiment_test2.append(corpus_sentiment_test[i])\n",
    "        \n",
    "train_dataset2 = EmbeddingSet(embeding_dictionary, corpus_text_train2, corpus_sentiment_train2, transform, target_transform)\n",
    "test_dataset2 = EmbeddingSet(embeding_dictionary, corpus_text_test2, corpus_sentiment_test2, transform, target_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5c2918d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4869\n",
      "20231\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "train_loader2 = DataLoader(train_dataset2,batch_size = batch_size,shuffle = True)\n",
    "batch_size2 = 1\n",
    "test_loader2 = DataLoader(test_dataset2,batch_size = batch_size2,shuffle = False)\n",
    "print(len(test_loader2))\n",
    "print(len(train_loader2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a75aecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMMLP(nn.Module):\n",
    "    def __init__(self, inputsize):\n",
    "        super(SVMMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputsize, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 512)\n",
    "        self.fc3 = nn.Linear(512, 128)\n",
    "        self.fc4 = nn.Linear(128, 32)\n",
    "        self.fc5 = nn.Linear(32, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.fc1(input)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x) \n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x) \n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x) \n",
    "        x = self.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ae3c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "svmnetwork2 = SVMMLP(len(train_dataset2[0][0])).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9295696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(svmnetwork2.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31b3477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(X.float())\n",
    "\n",
    "        loss = loss_fn(pred,y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>11f}  [{current:>5d}/{size:>5d}]\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "571668f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X=X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred,y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    print(f\"Test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5956795e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "epoch 0\n",
      "loss:    0.935015  [    0/20231]\n",
      "loss:    1.191685  [  100/20231]\n",
      "loss:    1.187618  [  200/20231]\n",
      "loss:    0.930415  [  300/20231]\n",
      "loss:    0.924624  [  400/20231]\n",
      "loss:    0.927944  [  500/20231]\n",
      "loss:    1.250665  [  600/20231]\n",
      "loss:    0.909742  [  700/20231]\n",
      "loss:    0.931657  [  800/20231]\n",
      "loss:    1.121886  [  900/20231]\n",
      "loss:    1.289078  [ 1000/20231]\n",
      "loss:    1.002137  [ 1100/20231]\n",
      "loss:    1.282259  [ 1200/20231]\n",
      "loss:    1.146538  [ 1300/20231]\n",
      "loss:    1.094813  [ 1400/20231]\n",
      "loss:    1.258454  [ 1500/20231]\n",
      "loss:    1.164100  [ 1600/20231]\n",
      "loss:    1.265068  [ 1700/20231]\n",
      "loss:    1.141715  [ 1800/20231]\n",
      "loss:    0.934202  [ 1900/20231]\n",
      "loss:    1.252279  [ 2000/20231]\n",
      "loss:    1.256956  [ 2100/20231]\n",
      "loss:    1.259976  [ 2200/20231]\n",
      "loss:    1.300965  [ 2300/20231]\n",
      "loss:    1.314000  [ 2400/20231]\n",
      "loss:    1.094827  [ 2500/20231]\n",
      "loss:    1.333444  [ 2600/20231]\n",
      "loss:    0.932487  [ 2700/20231]\n",
      "loss:    1.255218  [ 2800/20231]\n",
      "loss:    1.092344  [ 2900/20231]\n",
      "loss:    1.210861  [ 3000/20231]\n",
      "loss:    1.114838  [ 3100/20231]\n",
      "loss:    0.939546  [ 3200/20231]\n",
      "loss:    1.060461  [ 3300/20231]\n",
      "loss:    0.949919  [ 3400/20231]\n",
      "loss:    1.261283  [ 3500/20231]\n",
      "loss:    0.955826  [ 3600/20231]\n",
      "loss:    1.132722  [ 3700/20231]\n",
      "loss:    1.281761  [ 3800/20231]\n",
      "loss:    1.138352  [ 3900/20231]\n",
      "loss:    0.935784  [ 4000/20231]\n",
      "loss:    1.238719  [ 4100/20231]\n",
      "loss:    0.954828  [ 4200/20231]\n",
      "loss:    1.265038  [ 4300/20231]\n",
      "loss:    1.133616  [ 4400/20231]\n",
      "loss:    0.943297  [ 4500/20231]\n",
      "loss:    1.120265  [ 4600/20231]\n",
      "loss:    1.006062  [ 4700/20231]\n",
      "loss:    0.998741  [ 4800/20231]\n",
      "loss:    0.948490  [ 4900/20231]\n",
      "loss:    1.263358  [ 5000/20231]\n",
      "loss:    1.207520  [ 5100/20231]\n",
      "loss:    0.950440  [ 5200/20231]\n",
      "loss:    0.924796  [ 5300/20231]\n",
      "loss:    1.169171  [ 5400/20231]\n",
      "loss:    1.229232  [ 5500/20231]\n",
      "loss:    0.915617  [ 5600/20231]\n",
      "loss:    0.910252  [ 5700/20231]\n",
      "loss:    1.387219  [ 5800/20231]\n",
      "loss:    1.165692  [ 5900/20231]\n",
      "loss:    0.996896  [ 6000/20231]\n",
      "loss:    1.257551  [ 6100/20231]\n",
      "loss:    0.870451  [ 6200/20231]\n",
      "loss:    1.423421  [ 6300/20231]\n",
      "loss:    0.935582  [ 6400/20231]\n",
      "loss:    1.141770  [ 6500/20231]\n",
      "loss:    1.518857  [ 6600/20231]\n",
      "loss:    0.933934  [ 6700/20231]\n",
      "loss:    1.315477  [ 6800/20231]\n",
      "loss:    0.922482  [ 6900/20231]\n",
      "loss:    1.228058  [ 7000/20231]\n",
      "loss:    0.855674  [ 7100/20231]\n",
      "loss:    0.895710  [ 7200/20231]\n",
      "loss:    1.302082  [ 7300/20231]\n",
      "loss:    1.204142  [ 7400/20231]\n",
      "loss:    1.054245  [ 7500/20231]\n",
      "loss:    1.187330  [ 7600/20231]\n",
      "loss:    1.341275  [ 7700/20231]\n",
      "loss:    1.216624  [ 7800/20231]\n",
      "loss:    0.410459  [ 7900/20231]\n",
      "loss:    1.201269  [ 8000/20231]\n",
      "loss:    1.318543  [ 8100/20231]\n",
      "loss:    0.976793  [ 8200/20231]\n",
      "loss:    0.957499  [ 8300/20231]\n",
      "loss:    1.394584  [ 8400/20231]\n",
      "loss:    0.900299  [ 8500/20231]\n",
      "loss:    0.914156  [ 8600/20231]\n",
      "loss:    1.234550  [ 8700/20231]\n",
      "loss:    1.000694  [ 8800/20231]\n",
      "loss:    1.044757  [ 8900/20231]\n",
      "loss:    1.214418  [ 9000/20231]\n",
      "loss:    1.325035  [ 9100/20231]\n",
      "loss:    0.943796  [ 9200/20231]\n",
      "loss:    0.910832  [ 9300/20231]\n",
      "loss:    1.086884  [ 9400/20231]\n",
      "loss:    1.131472  [ 9500/20231]\n",
      "loss:    1.142750  [ 9600/20231]\n",
      "loss:    1.041220  [ 9700/20231]\n",
      "loss:    0.867966  [ 9800/20231]\n",
      "loss:    1.117913  [ 9900/20231]\n",
      "loss:    1.176856  [10000/20231]\n",
      "loss:    0.985562  [10100/20231]\n",
      "loss:    0.806363  [10200/20231]\n",
      "loss:    0.928333  [10300/20231]\n",
      "loss:    0.827323  [10400/20231]\n",
      "loss:    0.870876  [10500/20231]\n",
      "loss:    0.747354  [10600/20231]\n",
      "loss:    0.936408  [10700/20231]\n",
      "loss:    0.828448  [10800/20231]\n",
      "loss:    1.164104  [10900/20231]\n",
      "loss:    0.979320  [11000/20231]\n",
      "loss:    0.886598  [11100/20231]\n",
      "loss:    1.438212  [11200/20231]\n",
      "loss:    1.009244  [11300/20231]\n",
      "loss:    0.855281  [11400/20231]\n",
      "loss:    1.426186  [11500/20231]\n",
      "loss:    1.228225  [11600/20231]\n",
      "loss:    1.135234  [11700/20231]\n",
      "loss:    0.874212  [11800/20231]\n",
      "loss:    0.832265  [11900/20231]\n",
      "loss:    1.084920  [12000/20231]\n",
      "loss:    0.849121  [12100/20231]\n",
      "loss:    0.857430  [12200/20231]\n",
      "loss:    0.856649  [12300/20231]\n",
      "loss:    0.940107  [12400/20231]\n",
      "loss:    0.959843  [12500/20231]\n",
      "loss:    1.301810  [12600/20231]\n",
      "loss:    1.053894  [12700/20231]\n",
      "loss:    0.950012  [12800/20231]\n",
      "loss:    1.243409  [12900/20231]\n",
      "loss:    0.852743  [13000/20231]\n",
      "loss:    1.530016  [13100/20231]\n",
      "loss:    1.205036  [13200/20231]\n",
      "loss:    1.029406  [13300/20231]\n",
      "loss:    1.223616  [13400/20231]\n",
      "loss:    0.754609  [13500/20231]\n",
      "loss:    1.078623  [13600/20231]\n",
      "loss:    1.370160  [13700/20231]\n",
      "loss:    0.795486  [13800/20231]\n",
      "loss:    1.406413  [13900/20231]\n",
      "loss:    0.821827  [14000/20231]\n",
      "loss:    1.474253  [14100/20231]\n",
      "loss:    1.206549  [14200/20231]\n",
      "loss:    0.844583  [14300/20231]\n",
      "loss:    1.181916  [14400/20231]\n",
      "loss:    0.822731  [14500/20231]\n",
      "loss:    0.976659  [14600/20231]\n",
      "loss:    1.100615  [14700/20231]\n",
      "loss:    0.789861  [14800/20231]\n",
      "loss:    1.206084  [14900/20231]\n",
      "loss:    0.931212  [15000/20231]\n",
      "loss:    0.771695  [15100/20231]\n",
      "loss:    0.722067  [15200/20231]\n",
      "loss:    0.872108  [15300/20231]\n",
      "loss:    1.076842  [15400/20231]\n",
      "loss:    0.800077  [15500/20231]\n",
      "loss:    1.038366  [15600/20231]\n",
      "loss:    0.750445  [15700/20231]\n",
      "loss:    0.762144  [15800/20231]\n",
      "loss:    1.221939  [15900/20231]\n",
      "loss:    1.471918  [16000/20231]\n",
      "loss:    0.956744  [16100/20231]\n",
      "loss:    0.737669  [16200/20231]\n",
      "loss:    0.834342  [16300/20231]\n",
      "loss:    0.875036  [16400/20231]\n",
      "loss:    1.162633  [16500/20231]\n",
      "loss:    1.032662  [16600/20231]\n",
      "loss:    1.126559  [16700/20231]\n",
      "loss:    1.075853  [16800/20231]\n",
      "loss:    0.682388  [16900/20231]\n",
      "loss:    0.757885  [17000/20231]\n",
      "loss:    1.055377  [17100/20231]\n",
      "loss:    1.681055  [17200/20231]\n",
      "loss:    0.895032  [17300/20231]\n",
      "loss:    0.609629  [17400/20231]\n",
      "loss:    0.870356  [17500/20231]\n",
      "loss:    1.453004  [17600/20231]\n",
      "loss:    0.982771  [17700/20231]\n",
      "loss:    0.655062  [17800/20231]\n",
      "loss:    1.094687  [17900/20231]\n",
      "loss:    0.923757  [18000/20231]\n",
      "loss:    0.972246  [18100/20231]\n",
      "loss:    0.791997  [18200/20231]\n",
      "loss:    0.912524  [18300/20231]\n",
      "loss:    0.664064  [18400/20231]\n",
      "loss:    0.866733  [18500/20231]\n",
      "loss:    1.381883  [18600/20231]\n",
      "loss:    0.771690  [18700/20231]\n",
      "loss:    1.181205  [18800/20231]\n",
      "loss:    0.839622  [18900/20231]\n",
      "loss:    1.827412  [19000/20231]\n",
      "loss:    0.904461  [19100/20231]\n",
      "loss:    0.956231  [19200/20231]\n",
      "loss:    0.582410  [19300/20231]\n",
      "loss:    0.824923  [19400/20231]\n",
      "loss:    1.109125  [19500/20231]\n",
      "loss:    0.822635  [19600/20231]\n",
      "loss:    0.897685  [19700/20231]\n",
      "loss:    0.727674  [19800/20231]\n",
      "loss:    0.867191  [19900/20231]\n",
      "loss:    0.388601  [20000/20231]\n",
      "loss:    1.008532  [20100/20231]\n",
      "loss:    1.628783  [20200/20231]\n",
      "Test: \n",
      " Accuracy: 46.8%, Avg loss: 1.065948 \n",
      "\n",
      "======================================================\n",
      "epoch 1\n",
      "loss:    0.839703  [    0/20231]\n",
      "loss:    1.085846  [  100/20231]\n",
      "loss:    1.028033  [  200/20231]\n",
      "loss:    1.007990  [  300/20231]\n",
      "loss:    1.321436  [  400/20231]\n",
      "loss:    0.958825  [  500/20231]\n",
      "loss:    1.124149  [  600/20231]\n",
      "loss:    1.054111  [  700/20231]\n",
      "loss:    0.662007  [  800/20231]\n",
      "loss:    0.599760  [  900/20231]\n",
      "loss:    0.558111  [ 1000/20231]\n",
      "loss:    1.243268  [ 1100/20231]\n",
      "loss:    1.409010  [ 1200/20231]\n",
      "loss:    0.740305  [ 1300/20231]\n",
      "loss:    1.725272  [ 1400/20231]\n",
      "loss:    0.683997  [ 1500/20231]\n",
      "loss:    0.865587  [ 1600/20231]\n",
      "loss:    0.548357  [ 1700/20231]\n",
      "loss:    1.060325  [ 1800/20231]\n",
      "loss:    1.630592  [ 1900/20231]\n",
      "loss:    0.522203  [ 2000/20231]\n",
      "loss:    1.158881  [ 2100/20231]\n",
      "loss:    0.380985  [ 2200/20231]\n",
      "loss:    0.484610  [ 2300/20231]\n",
      "loss:    0.320885  [ 2400/20231]\n",
      "loss:    1.256210  [ 2500/20231]\n",
      "loss:    1.042068  [ 2600/20231]\n",
      "loss:    1.292657  [ 2700/20231]\n",
      "loss:    0.063307  [ 2800/20231]\n",
      "loss:    0.739251  [ 2900/20231]\n",
      "loss:    0.725992  [ 3000/20231]\n",
      "loss:    1.494669  [ 3100/20231]\n",
      "loss:    0.026966  [ 3200/20231]\n",
      "loss:    1.178211  [ 3300/20231]\n",
      "loss:    0.577144  [ 3400/20231]\n",
      "loss:    0.971211  [ 3500/20231]\n",
      "loss:    0.861399  [ 3600/20231]\n",
      "loss:    1.254594  [ 3700/20231]\n",
      "loss:    0.793291  [ 3800/20231]\n",
      "loss:    0.857983  [ 3900/20231]\n",
      "loss:    1.282388  [ 4000/20231]\n",
      "loss:    0.726622  [ 4100/20231]\n",
      "loss:    1.309822  [ 4200/20231]\n",
      "loss:    0.825584  [ 4300/20231]\n",
      "loss:    0.507328  [ 4400/20231]\n",
      "loss:    1.681136  [ 4500/20231]\n",
      "loss:    1.050719  [ 4600/20231]\n",
      "loss:    0.777649  [ 4700/20231]\n",
      "loss:    1.214345  [ 4800/20231]\n",
      "loss:    0.647728  [ 4900/20231]\n",
      "loss:    0.755809  [ 5000/20231]\n",
      "loss:    0.699877  [ 5100/20231]\n",
      "loss:    0.954344  [ 5200/20231]\n",
      "loss:    1.459226  [ 5300/20231]\n",
      "loss:    0.904485  [ 5400/20231]\n",
      "loss:    0.899963  [ 5500/20231]\n",
      "loss:    0.636541  [ 5600/20231]\n",
      "loss:    0.817109  [ 5700/20231]\n",
      "loss:    1.001786  [ 5800/20231]\n",
      "loss:    0.263249  [ 5900/20231]\n",
      "loss:    0.572971  [ 6000/20231]\n",
      "loss:    0.993684  [ 6100/20231]\n",
      "loss:    1.762975  [ 6200/20231]\n",
      "loss:    0.612831  [ 6300/20231]\n",
      "loss:    1.873353  [ 6400/20231]\n",
      "loss:    0.638962  [ 6500/20231]\n",
      "loss:    0.399713  [ 6600/20231]\n",
      "loss:    2.636339  [ 6700/20231]\n",
      "loss:    0.909255  [ 6800/20231]\n",
      "loss:    1.084886  [ 6900/20231]\n",
      "loss:    1.383099  [ 7000/20231]\n",
      "loss:    1.381511  [ 7100/20231]\n",
      "loss:    0.393148  [ 7200/20231]\n",
      "loss:    1.086670  [ 7300/20231]\n",
      "loss:    0.358132  [ 7400/20231]\n",
      "loss:    1.830821  [ 7500/20231]\n",
      "loss:    1.310294  [ 7600/20231]\n",
      "loss:    0.956093  [ 7700/20231]\n",
      "loss:    0.754235  [ 7800/20231]\n",
      "loss:    0.843332  [ 7900/20231]\n",
      "loss:    0.816744  [ 8000/20231]\n",
      "loss:    0.399607  [ 8100/20231]\n",
      "loss:    0.762937  [ 8200/20231]\n",
      "loss:    1.188021  [ 8300/20231]\n",
      "loss:    0.809531  [ 8400/20231]\n",
      "loss:    0.672940  [ 8500/20231]\n",
      "loss:    0.918379  [ 8600/20231]\n",
      "loss:    1.080693  [ 8700/20231]\n",
      "loss:    0.513327  [ 8800/20231]\n",
      "loss:    1.717695  [ 8900/20231]\n",
      "loss:    0.948198  [ 9000/20231]\n",
      "loss:    1.468166  [ 9100/20231]\n",
      "loss:    1.596691  [ 9200/20231]\n",
      "loss:    0.693002  [ 9300/20231]\n",
      "loss:    0.622688  [ 9400/20231]\n",
      "loss:    1.350875  [ 9500/20231]\n",
      "loss:    1.042629  [ 9600/20231]\n",
      "loss:    1.641562  [ 9700/20231]\n",
      "loss:    0.952973  [ 9800/20231]\n",
      "loss:    1.119455  [ 9900/20231]\n",
      "loss:    1.424343  [10000/20231]\n",
      "loss:    1.689863  [10100/20231]\n",
      "loss:    1.157516  [10200/20231]\n",
      "loss:    0.716082  [10300/20231]\n",
      "loss:    1.311296  [10400/20231]\n",
      "loss:    0.089962  [10500/20231]\n",
      "loss:    1.591281  [10600/20231]\n",
      "loss:    0.308951  [10700/20231]\n",
      "loss:    1.010243  [10800/20231]\n",
      "loss:    1.311542  [10900/20231]\n",
      "loss:    1.082759  [11000/20231]\n",
      "loss:    1.044449  [11100/20231]\n",
      "loss:    1.027999  [11200/20231]\n",
      "loss:    0.174889  [11300/20231]\n",
      "loss:    1.250318  [11400/20231]\n",
      "loss:    1.178522  [11500/20231]\n",
      "loss:    1.315927  [11600/20231]\n",
      "loss:    0.255695  [11700/20231]\n",
      "loss:    0.422385  [11800/20231]\n",
      "loss:    0.280853  [11900/20231]\n",
      "loss:    1.332254  [12000/20231]\n",
      "loss:    0.533021  [12100/20231]\n",
      "loss:    1.035331  [12200/20231]\n",
      "loss:    0.568944  [12300/20231]\n",
      "loss:    0.403718  [12400/20231]\n",
      "loss:    1.417332  [12500/20231]\n",
      "loss:    1.148360  [12600/20231]\n",
      "loss:    1.262103  [12700/20231]\n",
      "loss:    1.364463  [12800/20231]\n",
      "loss:    0.348139  [12900/20231]\n",
      "loss:    0.520159  [13000/20231]\n",
      "loss:    1.008318  [13100/20231]\n",
      "loss:    0.798083  [13200/20231]\n",
      "loss:    1.446775  [13300/20231]\n",
      "loss:    0.393200  [13400/20231]\n",
      "loss:    1.758535  [13500/20231]\n",
      "loss:    0.485549  [13600/20231]\n",
      "loss:    1.001465  [13700/20231]\n",
      "loss:    0.870175  [13800/20231]\n",
      "loss:    1.094402  [13900/20231]\n",
      "loss:    2.223099  [14000/20231]\n",
      "loss:    0.878578  [14100/20231]\n",
      "loss:    0.654566  [14200/20231]\n",
      "loss:    0.484092  [14300/20231]\n",
      "loss:    0.768417  [14400/20231]\n",
      "loss:    0.520753  [14500/20231]\n",
      "loss:    1.220618  [14600/20231]\n",
      "loss:    1.308872  [14700/20231]\n",
      "loss:    0.497004  [14800/20231]\n",
      "loss:    1.231407  [14900/20231]\n",
      "loss:    0.646089  [15000/20231]\n",
      "loss:    2.138856  [15100/20231]\n",
      "loss:    1.104742  [15200/20231]\n",
      "loss:    1.306793  [15300/20231]\n",
      "loss:    0.666519  [15400/20231]\n",
      "loss:    0.664311  [15500/20231]\n",
      "loss:    0.941216  [15600/20231]\n",
      "loss:    1.302564  [15700/20231]\n",
      "loss:    0.522022  [15800/20231]\n",
      "loss:    1.460734  [15900/20231]\n",
      "loss:    0.622661  [16000/20231]\n",
      "loss:    1.176296  [16100/20231]\n",
      "loss:    1.828792  [16200/20231]\n",
      "loss:    0.755989  [16300/20231]\n",
      "loss:    0.661419  [16400/20231]\n",
      "loss:    1.194865  [16500/20231]\n",
      "loss:    1.245363  [16600/20231]\n",
      "loss:    1.839396  [16700/20231]\n",
      "loss:    0.633367  [16800/20231]\n",
      "loss:    1.563349  [16900/20231]\n",
      "loss:    1.228977  [17000/20231]\n",
      "loss:    1.308285  [17100/20231]\n",
      "loss:    1.132675  [17200/20231]\n",
      "loss:    1.458644  [17300/20231]\n",
      "loss:    0.629908  [17400/20231]\n",
      "loss:    0.886845  [17500/20231]\n",
      "loss:    0.610546  [17600/20231]\n",
      "loss:    0.906876  [17700/20231]\n",
      "loss:    1.077912  [17800/20231]\n",
      "loss:    1.328784  [17900/20231]\n",
      "loss:    0.495358  [18000/20231]\n",
      "loss:    0.851003  [18100/20231]\n",
      "loss:    0.949794  [18200/20231]\n",
      "loss:    0.332795  [18300/20231]\n",
      "loss:    0.896212  [18400/20231]\n",
      "loss:    0.640283  [18500/20231]\n",
      "loss:    0.941892  [18600/20231]\n",
      "loss:    1.409070  [18700/20231]\n",
      "loss:    1.052675  [18800/20231]\n",
      "loss:    1.282065  [18900/20231]\n",
      "loss:    0.874245  [19000/20231]\n",
      "loss:    1.779596  [19100/20231]\n",
      "loss:    1.075125  [19200/20231]\n",
      "loss:    1.194588  [19300/20231]\n",
      "loss:    0.147559  [19400/20231]\n",
      "loss:    0.455096  [19500/20231]\n",
      "loss:    0.614900  [19600/20231]\n",
      "loss:    1.568533  [19700/20231]\n",
      "loss:    1.536017  [19800/20231]\n",
      "loss:    1.201668  [19900/20231]\n",
      "loss:    0.533510  [20000/20231]\n",
      "loss:    1.485481  [20100/20231]\n",
      "loss:    1.172746  [20200/20231]\n",
      "Test: \n",
      " Accuracy: 46.1%, Avg loss: 1.070366 \n",
      "\n",
      "======================================================\n",
      "epoch 2\n",
      "loss:    0.436546  [    0/20231]\n",
      "loss:    1.197772  [  100/20231]\n",
      "loss:    0.795430  [  200/20231]\n",
      "loss:    0.983604  [  300/20231]\n",
      "loss:    0.862158  [  400/20231]\n",
      "loss:    0.900250  [  500/20231]\n",
      "loss:    0.744743  [  600/20231]\n",
      "loss:    1.166127  [  700/20231]\n",
      "loss:    1.262217  [  800/20231]\n",
      "loss:    1.165067  [  900/20231]\n",
      "loss:    1.408059  [ 1000/20231]\n",
      "loss:    1.390676  [ 1100/20231]\n",
      "loss:    0.222170  [ 1200/20231]\n",
      "loss:    0.072925  [ 1300/20231]\n",
      "loss:    1.257162  [ 1400/20231]\n",
      "loss:    0.486592  [ 1500/20231]\n",
      "loss:    0.396909  [ 1600/20231]\n",
      "loss:    0.444127  [ 1700/20231]\n",
      "loss:    0.591121  [ 1800/20231]\n",
      "loss:    0.094081  [ 1900/20231]\n",
      "loss:    0.315053  [ 2000/20231]\n",
      "loss:    1.193473  [ 2100/20231]\n",
      "loss:    1.071614  [ 2200/20231]\n",
      "loss:    0.741213  [ 2300/20231]\n",
      "loss:    0.714986  [ 2400/20231]\n",
      "loss:    0.958393  [ 2500/20231]\n",
      "loss:    0.441120  [ 2600/20231]\n",
      "loss:    0.216239  [ 2700/20231]\n",
      "loss:    1.960585  [ 2800/20231]\n",
      "loss:    1.575479  [ 2900/20231]\n",
      "loss:    0.938056  [ 3000/20231]\n",
      "loss:    1.076001  [ 3100/20231]\n",
      "loss:    1.298506  [ 3200/20231]\n",
      "loss:    1.878436  [ 3300/20231]\n",
      "loss:    0.865654  [ 3400/20231]\n",
      "loss:    1.107598  [ 3500/20231]\n",
      "loss:    0.275715  [ 3600/20231]\n",
      "loss:    0.979484  [ 3700/20231]\n",
      "loss:    1.122223  [ 3800/20231]\n",
      "loss:    0.920355  [ 3900/20231]\n",
      "loss:    0.334848  [ 4000/20231]\n",
      "loss:    0.215781  [ 4100/20231]\n",
      "loss:    0.578570  [ 4200/20231]\n",
      "loss:    0.312933  [ 4300/20231]\n",
      "loss:    0.403610  [ 4400/20231]\n",
      "loss:    0.388631  [ 4500/20231]\n",
      "loss:    1.689850  [ 4600/20231]\n",
      "loss:    1.150651  [ 4700/20231]\n",
      "loss:    0.314453  [ 4800/20231]\n",
      "loss:    0.603530  [ 4900/20231]\n",
      "loss:    1.221962  [ 5000/20231]\n",
      "loss:    0.637108  [ 5100/20231]\n",
      "loss:    0.804740  [ 5200/20231]\n",
      "loss:    0.928454  [ 5300/20231]\n",
      "loss:    0.604187  [ 5400/20231]\n",
      "loss:    0.750258  [ 5500/20231]\n",
      "loss:    0.721290  [ 5600/20231]\n",
      "loss:    1.084447  [ 5700/20231]\n",
      "loss:    0.508110  [ 5800/20231]\n",
      "loss:    1.299614  [ 5900/20231]\n",
      "loss:    1.318286  [ 6000/20231]\n",
      "loss:    1.077222  [ 6100/20231]\n",
      "loss:    0.895356  [ 6200/20231]\n",
      "loss:    0.759757  [ 6300/20231]\n",
      "loss:    0.520856  [ 6400/20231]\n",
      "loss:    1.335668  [ 6500/20231]\n",
      "loss:    2.859639  [ 6600/20231]\n",
      "loss:    0.476679  [ 6700/20231]\n",
      "loss:    1.409467  [ 6800/20231]\n",
      "loss:    1.310057  [ 6900/20231]\n",
      "loss:    0.361652  [ 7000/20231]\n",
      "loss:    0.504001  [ 7100/20231]\n",
      "loss:    0.993990  [ 7200/20231]\n",
      "loss:    1.275203  [ 7300/20231]\n",
      "loss:    0.767077  [ 7400/20231]\n",
      "loss:    0.769011  [ 7500/20231]\n",
      "loss:    0.979412  [ 7600/20231]\n",
      "loss:    0.699347  [ 7700/20231]\n",
      "loss:    0.725147  [ 7800/20231]\n",
      "loss:    1.809870  [ 7900/20231]\n",
      "loss:    0.784058  [ 8000/20231]\n",
      "loss:    0.491660  [ 8100/20231]\n",
      "loss:    0.229910  [ 8200/20231]\n",
      "loss:    0.645048  [ 8300/20231]\n",
      "loss:    0.924015  [ 8400/20231]\n",
      "loss:    1.880037  [ 8500/20231]\n",
      "loss:    1.149806  [ 8600/20231]\n",
      "loss:    0.685600  [ 8700/20231]\n",
      "loss:    0.779505  [ 8800/20231]\n",
      "loss:    0.972000  [ 8900/20231]\n",
      "loss:    0.395174  [ 9000/20231]\n",
      "loss:    0.632584  [ 9100/20231]\n",
      "loss:    1.082684  [ 9200/20231]\n",
      "loss:    0.678739  [ 9300/20231]\n",
      "loss:    0.435301  [ 9400/20231]\n",
      "loss:    0.458256  [ 9500/20231]\n",
      "loss:    1.772908  [ 9600/20231]\n",
      "loss:    1.178793  [ 9700/20231]\n",
      "loss:    0.190917  [ 9800/20231]\n",
      "loss:    0.437579  [ 9900/20231]\n",
      "loss:    0.231170  [10000/20231]\n",
      "loss:    0.494379  [10100/20231]\n",
      "loss:    0.874817  [10200/20231]\n",
      "loss:    0.309319  [10300/20231]\n",
      "loss:    0.702035  [10400/20231]\n",
      "loss:    1.818229  [10500/20231]\n",
      "loss:    0.780725  [10600/20231]\n",
      "loss:    0.278192  [10700/20231]\n",
      "loss:    1.174887  [10800/20231]\n",
      "loss:    1.545436  [10900/20231]\n",
      "loss:    0.414973  [11000/20231]\n",
      "loss:    0.362378  [11100/20231]\n",
      "loss:    0.950910  [11200/20231]\n",
      "loss:    0.169003  [11300/20231]\n",
      "loss:    0.927909  [11400/20231]\n",
      "loss:    0.306687  [11500/20231]\n",
      "loss:    0.377354  [11600/20231]\n",
      "loss:    0.445975  [11700/20231]\n",
      "loss:    0.710821  [11800/20231]\n",
      "loss:    0.674862  [11900/20231]\n",
      "loss:    1.312791  [12000/20231]\n",
      "loss:    0.411316  [12100/20231]\n",
      "loss:    1.132370  [12200/20231]\n",
      "loss:    0.389650  [12300/20231]\n",
      "loss:    0.440718  [12400/20231]\n",
      "loss:    0.778312  [12500/20231]\n",
      "loss:    0.435786  [12600/20231]\n",
      "loss:    0.245144  [12700/20231]\n",
      "loss:    1.617038  [12800/20231]\n",
      "loss:    0.804784  [12900/20231]\n",
      "loss:    0.919565  [13000/20231]\n",
      "loss:    1.525235  [13100/20231]\n",
      "loss:    0.277452  [13200/20231]\n",
      "loss:    0.980607  [13300/20231]\n",
      "loss:    0.653712  [13400/20231]\n",
      "loss:    0.078090  [13500/20231]\n",
      "loss:    0.274373  [13600/20231]\n",
      "loss:    1.818671  [13700/20231]\n",
      "loss:    0.937543  [13800/20231]\n",
      "loss:    1.919673  [13900/20231]\n",
      "loss:    0.264615  [14000/20231]\n",
      "loss:    0.445846  [14100/20231]\n",
      "loss:    0.195697  [14200/20231]\n",
      "loss:    1.096760  [14300/20231]\n",
      "loss:    0.397616  [14400/20231]\n",
      "loss:    0.786705  [14500/20231]\n",
      "loss:    0.877718  [14600/20231]\n",
      "loss:    0.122652  [14700/20231]\n",
      "loss:    1.410105  [14800/20231]\n",
      "loss:    1.701483  [14900/20231]\n",
      "loss:    1.354050  [15000/20231]\n",
      "loss:    0.647624  [15100/20231]\n",
      "loss:    0.510129  [15200/20231]\n",
      "loss:    0.760325  [15300/20231]\n",
      "loss:    0.808172  [15400/20231]\n",
      "loss:    0.295257  [15500/20231]\n",
      "loss:    0.147906  [15600/20231]\n",
      "loss:    1.106468  [15700/20231]\n",
      "loss:    0.339351  [15800/20231]\n",
      "loss:    0.889758  [15900/20231]\n",
      "loss:    0.373571  [16000/20231]\n",
      "loss:    1.849796  [16100/20231]\n",
      "loss:    0.531843  [16200/20231]\n",
      "loss:    1.764881  [16300/20231]\n",
      "loss:    1.041642  [16400/20231]\n",
      "loss:    0.390315  [16500/20231]\n",
      "loss:    0.509676  [16600/20231]\n",
      "loss:    0.988476  [16700/20231]\n",
      "loss:    1.550339  [16800/20231]\n",
      "loss:    0.693712  [16900/20231]\n",
      "loss:    0.318462  [17000/20231]\n",
      "loss:    0.978058  [17100/20231]\n",
      "loss:    0.918423  [17200/20231]\n",
      "loss:    0.847490  [17300/20231]\n",
      "loss:    0.325706  [17400/20231]\n",
      "loss:    1.274576  [17500/20231]\n",
      "loss:    0.584297  [17600/20231]\n",
      "loss:    0.500366  [17700/20231]\n",
      "loss:    0.296362  [17800/20231]\n",
      "loss:    0.315414  [17900/20231]\n",
      "loss:    2.143453  [18000/20231]\n",
      "loss:    1.302346  [18100/20231]\n",
      "loss:    0.333133  [18200/20231]\n",
      "loss:    0.928799  [18300/20231]\n",
      "loss:    0.184309  [18400/20231]\n",
      "loss:    1.475507  [18500/20231]\n",
      "loss:    0.689026  [18600/20231]\n",
      "loss:    0.656331  [18700/20231]\n",
      "loss:    0.925841  [18800/20231]\n",
      "loss:    0.846183  [18900/20231]\n",
      "loss:    0.402209  [19000/20231]\n",
      "loss:    0.829488  [19100/20231]\n",
      "loss:    1.203201  [19200/20231]\n",
      "loss:    0.881720  [19300/20231]\n",
      "loss:    0.318428  [19400/20231]\n",
      "loss:    0.439297  [19500/20231]\n",
      "loss:    0.185187  [19600/20231]\n",
      "loss:    1.445164  [19700/20231]\n",
      "loss:    0.823354  [19800/20231]\n",
      "loss:    0.293230  [19900/20231]\n",
      "loss:    0.534960  [20000/20231]\n",
      "loss:    0.901516  [20100/20231]\n",
      "loss:    1.461380  [20200/20231]\n",
      "Test: \n",
      " Accuracy: 47.9%, Avg loss: 1.072074 \n",
      "\n",
      "======================================================\n",
      "epoch 3\n",
      "loss:    0.442495  [    0/20231]\n",
      "loss:    0.435107  [  100/20231]\n",
      "loss:    0.585185  [  200/20231]\n",
      "loss:    0.876398  [  300/20231]\n",
      "loss:    0.045968  [  400/20231]\n",
      "loss:    0.135007  [  500/20231]\n",
      "loss:    1.662979  [  600/20231]\n",
      "loss:    0.372753  [  700/20231]\n",
      "loss:    0.316091  [  800/20231]\n",
      "loss:    2.950680  [  900/20231]\n",
      "loss:    0.541556  [ 1000/20231]\n",
      "loss:    0.568991  [ 1100/20231]\n",
      "loss:    2.242178  [ 1200/20231]\n",
      "loss:    0.874024  [ 1300/20231]\n",
      "loss:    2.324557  [ 1400/20231]\n",
      "loss:    1.031925  [ 1500/20231]\n",
      "loss:    1.806681  [ 1600/20231]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15304\\2118764796.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvmnetwork2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mtest_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvmnetwork2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15304\\1009997982.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('======================================================')\n",
    "    print('epoch '+str(i))\n",
    "    \n",
    "    train_loop(train_loader2, svmnetwork2, criterion, optimizer)\n",
    "    test_loop(test_loader2, svmnetwork2, criterion)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f2c87e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(svmnetwork2, \"SVMmodel.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc3176f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3b2f5a1",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45413e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = open('glove.6B.50d.txt', encoding=\"utf8\")\n",
    "embeding_dictionary = {}\n",
    "for line in txt_file:\n",
    "    words = line.strip().split(' ')\n",
    "    w = [float(x) for x in [*words[1:]]]\n",
    "    embeding_dictionary[words[0]] = w\n",
    "\n",
    "txt_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64dc8d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding(vectors, leng, shape):\n",
    "    for i in range(leng-len(vectors)):\n",
    "        \n",
    "        vectors = torch.cat((vectors, torch.zeros(1,shape)))\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51edace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxl = 24\n",
    "class GloVeEmbeddingSet(Dataset):    \n",
    "    def __init__(self, glove_dictionary, text, sentiment, transform=None, target_transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.sentiment = torch.tensor(sentiment)\n",
    "        self.text = text\n",
    "        self.glove_dictionary = glove_dictionary\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentiment)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.X = torch.tensor([])      \n",
    "\n",
    "        \n",
    "        temp = []\n",
    "        for w in self.text[idx].split(' '):\n",
    "            if w in self.glove_dictionary:\n",
    "                self.X = torch.cat((self.X, torch.tensor(self.glove_dictionary[w]).reshape(1,-1)), 0)\n",
    "        \n",
    "        \n",
    "        self.X = zero_padding(self.X, maxl, 50)\n",
    "        #print(self.X.shape)\n",
    "        \n",
    "        self.y = self.sentiment[idx]\n",
    "        #self.X =  temp\n",
    "        \n",
    "        \n",
    "        return self.X, self.y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "251ddb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "target_transform = transforms.ToTensor()\n",
    "corpus_sentiment_train = [int(x) for x in corpus_sentiment_train]\n",
    "\n",
    "glove_train_dataset = GloVeEmbeddingSet(embeding_dictionary, corpus_text_train, corpus_sentiment_train, transform,\n",
    "                                        target_transform)\n",
    "\n",
    "corpus_sentiment_test = [int(x) for x in corpus_sentiment_test]\n",
    "glove_test_dataset = GloVeEmbeddingSet(embeding_dictionary, corpus_text_test, corpus_sentiment_test, transform, target_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f645f12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5067\n",
      "20240\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "glove_train_loader = DataLoader(glove_train_dataset,batch_size = batch_size,shuffle = True)\n",
    "glove_test_loader = DataLoader(glove_test_dataset,batch_size = batch_size,shuffle = False)\n",
    "print(len(glove_test_loader))\n",
    "print(len(glove_train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98c25db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, n_features, hidden_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 3)\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        hidden, carry = torch.randn(1, len(X_batch), hidden_size).to(device), torch.randn(1, len(X_batch), hidden_size).to(device)\n",
    "        output, (hidden, carry) = self.lstm(X_batch, (hidden, carry))\n",
    "        return self.linear(output[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19135c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 50\n",
    "n_features = 50\n",
    "network_glove = LSTMClassifier(n_features, hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6cfc476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(network_glove.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "049c3a13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "epoch 0\n",
      "loss:    1.069263  [    0/20240]\n",
      "loss:    1.078519  [  100/20240]\n",
      "loss:    1.011724  [  200/20240]\n",
      "loss:    1.184547  [  300/20240]\n",
      "loss:    1.165164  [  400/20240]\n",
      "loss:    1.023875  [  500/20240]\n",
      "loss:    1.113568  [  600/20240]\n",
      "loss:    1.095052  [  700/20240]\n",
      "loss:    1.096788  [  800/20240]\n",
      "loss:    1.213474  [  900/20240]\n",
      "loss:    1.008193  [ 1000/20240]\n",
      "loss:    1.021959  [ 1100/20240]\n",
      "loss:    1.225369  [ 1200/20240]\n",
      "loss:    1.065101  [ 1300/20240]\n",
      "loss:    1.064667  [ 1400/20240]\n",
      "loss:    1.014936  [ 1500/20240]\n",
      "loss:    1.015061  [ 1600/20240]\n",
      "loss:    1.270620  [ 1700/20240]\n",
      "loss:    1.025389  [ 1800/20240]\n",
      "loss:    1.019625  [ 1900/20240]\n",
      "loss:    1.284326  [ 2000/20240]\n",
      "loss:    0.986905  [ 2100/20240]\n",
      "loss:    1.044201  [ 2200/20240]\n",
      "loss:    1.033126  [ 2300/20240]\n",
      "loss:    1.047377  [ 2400/20240]\n",
      "loss:    1.299248  [ 2500/20240]\n",
      "loss:    1.039205  [ 2600/20240]\n",
      "loss:    0.999944  [ 2700/20240]\n",
      "loss:    1.043051  [ 2800/20240]\n",
      "loss:    0.980233  [ 2900/20240]\n",
      "loss:    1.043595  [ 3000/20240]\n",
      "loss:    1.017957  [ 3100/20240]\n",
      "loss:    0.983546  [ 3200/20240]\n",
      "loss:    0.938904  [ 3300/20240]\n",
      "loss:    1.058062  [ 3400/20240]\n",
      "loss:    0.962180  [ 3500/20240]\n",
      "loss:    0.971640  [ 3600/20240]\n",
      "loss:    0.980071  [ 3700/20240]\n",
      "loss:    1.293911  [ 3800/20240]\n",
      "loss:    0.995611  [ 3900/20240]\n",
      "loss:    1.044124  [ 4000/20240]\n",
      "loss:    0.989043  [ 4100/20240]\n",
      "loss:    1.295261  [ 4200/20240]\n",
      "loss:    0.983743  [ 4300/20240]\n",
      "loss:    1.053415  [ 4400/20240]\n",
      "loss:    1.316137  [ 4500/20240]\n",
      "loss:    1.304141  [ 4600/20240]\n",
      "loss:    1.313155  [ 4700/20240]\n",
      "loss:    1.290367  [ 4800/20240]\n",
      "loss:    1.039884  [ 4900/20240]\n",
      "loss:    1.049886  [ 5000/20240]\n",
      "loss:    0.996242  [ 5100/20240]\n",
      "loss:    1.040707  [ 5200/20240]\n",
      "loss:    0.993516  [ 5300/20240]\n",
      "loss:    0.990867  [ 5400/20240]\n",
      "loss:    1.040260  [ 5500/20240]\n",
      "loss:    1.005533  [ 5600/20240]\n",
      "loss:    1.033876  [ 5700/20240]\n",
      "loss:    0.986261  [ 5800/20240]\n",
      "loss:    1.040690  [ 5900/20240]\n",
      "loss:    1.048393  [ 6000/20240]\n",
      "loss:    1.056067  [ 6100/20240]\n",
      "loss:    0.973528  [ 6200/20240]\n",
      "loss:    1.053653  [ 6300/20240]\n",
      "loss:    0.980208  [ 6400/20240]\n",
      "loss:    0.996438  [ 6500/20240]\n",
      "loss:    0.989279  [ 6600/20240]\n",
      "loss:    1.036673  [ 6700/20240]\n",
      "loss:    1.330676  [ 6800/20240]\n",
      "loss:    1.317899  [ 6900/20240]\n",
      "loss:    1.048312  [ 7000/20240]\n",
      "loss:    0.967512  [ 7100/20240]\n",
      "loss:    1.046371  [ 7200/20240]\n",
      "loss:    1.032944  [ 7300/20240]\n",
      "loss:    0.976622  [ 7400/20240]\n",
      "loss:    1.053653  [ 7500/20240]\n",
      "loss:    1.044678  [ 7600/20240]\n",
      "loss:    1.375365  [ 7700/20240]\n",
      "loss:    1.059222  [ 7800/20240]\n",
      "loss:    1.064401  [ 7900/20240]\n",
      "loss:    0.949066  [ 8000/20240]\n",
      "loss:    0.953875  [ 8100/20240]\n",
      "loss:    1.339585  [ 8200/20240]\n",
      "loss:    1.334146  [ 8300/20240]\n",
      "loss:    1.310027  [ 8400/20240]\n",
      "loss:    0.937791  [ 8500/20240]\n",
      "loss:    0.970214  [ 8600/20240]\n",
      "loss:    0.982169  [ 8700/20240]\n",
      "loss:    0.973361  [ 8800/20240]\n",
      "loss:    0.967557  [ 8900/20240]\n",
      "loss:    1.053299  [ 9000/20240]\n",
      "loss:    0.952541  [ 9100/20240]\n",
      "loss:    0.970471  [ 9200/20240]\n",
      "loss:    1.060853  [ 9300/20240]\n",
      "loss:    0.971850  [ 9400/20240]\n",
      "loss:    0.955991  [ 9500/20240]\n",
      "loss:    0.929189  [ 9600/20240]\n",
      "loss:    1.374137  [ 9700/20240]\n",
      "loss:    0.942284  [ 9800/20240]\n",
      "loss:    0.912554  [ 9900/20240]\n",
      "loss:    1.078406  [10000/20240]\n",
      "loss:    1.294033  [10100/20240]\n",
      "loss:    1.302355  [10200/20240]\n",
      "loss:    0.942032  [10300/20240]\n",
      "loss:    0.941517  [10400/20240]\n",
      "loss:    1.090184  [10500/20240]\n",
      "loss:    1.293092  [10600/20240]\n",
      "loss:    1.293571  [10700/20240]\n",
      "loss:    0.917105  [10800/20240]\n",
      "loss:    1.320940  [10900/20240]\n",
      "loss:    1.138085  [11000/20240]\n",
      "loss:    0.882431  [11100/20240]\n",
      "loss:    0.918651  [11200/20240]\n",
      "loss:    0.888604  [11300/20240]\n",
      "loss:    0.897976  [11400/20240]\n",
      "loss:    1.325064  [11500/20240]\n",
      "loss:    1.101289  [11600/20240]\n",
      "loss:    1.094307  [11700/20240]\n",
      "loss:    1.093443  [11800/20240]\n",
      "loss:    1.275336  [11900/20240]\n",
      "loss:    0.968352  [12000/20240]\n",
      "loss:    1.080513  [12100/20240]\n",
      "loss:    0.979453  [12200/20240]\n",
      "loss:    0.985884  [12300/20240]\n",
      "loss:    1.057332  [12400/20240]\n",
      "loss:    1.275467  [12500/20240]\n",
      "loss:    1.005663  [12600/20240]\n",
      "loss:    0.987196  [12700/20240]\n",
      "loss:    1.049686  [12800/20240]\n",
      "loss:    1.288665  [12900/20240]\n",
      "loss:    1.053378  [13000/20240]\n",
      "loss:    1.059409  [13100/20240]\n",
      "loss:    1.048614  [13200/20240]\n",
      "loss:    1.045942  [13300/20240]\n",
      "loss:    0.898306  [13400/20240]\n",
      "loss:    1.369121  [13500/20240]\n",
      "loss:    0.938037  [13600/20240]\n",
      "loss:    1.332799  [13700/20240]\n",
      "loss:    1.066276  [13800/20240]\n",
      "loss:    1.330594  [13900/20240]\n",
      "loss:    1.296078  [14000/20240]\n",
      "loss:    1.311255  [14100/20240]\n",
      "loss:    1.310936  [14200/20240]\n",
      "loss:    1.228289  [14300/20240]\n",
      "loss:    0.956508  [14400/20240]\n",
      "loss:    0.693152  [14500/20240]\n",
      "loss:    1.334068  [14600/20240]\n",
      "loss:    1.327870  [14700/20240]\n",
      "loss:    1.020448  [14800/20240]\n",
      "loss:    1.018467  [14900/20240]\n",
      "loss:    0.914338  [15000/20240]\n",
      "loss:    0.760189  [15100/20240]\n",
      "loss:    1.141196  [15200/20240]\n",
      "loss:    1.332319  [15300/20240]\n",
      "loss:    0.666432  [15400/20240]\n",
      "loss:    0.977744  [15500/20240]\n",
      "loss:    1.377981  [15600/20240]\n",
      "loss:    1.078982  [15700/20240]\n",
      "loss:    0.713440  [15800/20240]\n",
      "loss:    1.253798  [15900/20240]\n",
      "loss:    0.969434  [16000/20240]\n",
      "loss:    0.763237  [16100/20240]\n",
      "loss:    1.444603  [16200/20240]\n",
      "loss:    1.224280  [16300/20240]\n",
      "loss:    0.948678  [16400/20240]\n",
      "loss:    1.262730  [16500/20240]\n",
      "loss:    1.391987  [16600/20240]\n",
      "loss:    1.111644  [16700/20240]\n",
      "loss:    0.737210  [16800/20240]\n",
      "loss:    1.017905  [16900/20240]\n",
      "loss:    1.141085  [17000/20240]\n",
      "loss:    1.061348  [17100/20240]\n",
      "loss:    1.284474  [17200/20240]\n",
      "loss:    1.046655  [17300/20240]\n",
      "loss:    1.298524  [17400/20240]\n",
      "loss:    1.131092  [17500/20240]\n",
      "loss:    1.272970  [17600/20240]\n",
      "loss:    0.908382  [17700/20240]\n",
      "loss:    0.518775  [17800/20240]\n",
      "loss:    1.236300  [17900/20240]\n",
      "loss:    1.400814  [18000/20240]\n",
      "loss:    1.289558  [18100/20240]\n",
      "loss:    0.997948  [18200/20240]\n",
      "loss:    1.146166  [18300/20240]\n",
      "loss:    0.837073  [18400/20240]\n",
      "loss:    0.567388  [18500/20240]\n",
      "loss:    0.801104  [18600/20240]\n",
      "loss:    0.526461  [18700/20240]\n",
      "loss:    0.915760  [18800/20240]\n",
      "loss:    1.140299  [18900/20240]\n",
      "loss:    1.245828  [19000/20240]\n",
      "loss:    0.915994  [19100/20240]\n",
      "loss:    1.236548  [19200/20240]\n",
      "loss:    1.106638  [19300/20240]\n",
      "loss:    0.910710  [19400/20240]\n",
      "loss:    1.147041  [19500/20240]\n",
      "loss:    0.652793  [19600/20240]\n",
      "loss:    1.259719  [19700/20240]\n",
      "loss:    1.118212  [19800/20240]\n",
      "loss:    1.538691  [19900/20240]\n",
      "loss:    1.233558  [20000/20240]\n",
      "loss:    0.915465  [20100/20240]\n",
      "loss:    1.273540  [20200/20240]\n",
      "Test: \n",
      " Accuracy: 40.6%, Avg loss: 1.069209 \n",
      "\n",
      "======================================================\n",
      "epoch 1\n",
      "loss:    1.236344  [    0/20240]\n",
      "loss:    1.321974  [  100/20240]\n",
      "loss:    0.525764  [  200/20240]\n",
      "loss:    1.143023  [  300/20240]\n",
      "loss:    0.642653  [  400/20240]\n",
      "loss:    0.638937  [  500/20240]\n",
      "loss:    1.283894  [  600/20240]\n",
      "loss:    0.684074  [  700/20240]\n",
      "loss:    1.258118  [  800/20240]\n",
      "loss:    0.815140  [  900/20240]\n",
      "loss:    0.639853  [ 1000/20240]\n",
      "loss:    0.790786  [ 1100/20240]\n",
      "loss:    0.984213  [ 1200/20240]\n",
      "loss:    0.950924  [ 1300/20240]\n",
      "loss:    1.247660  [ 1400/20240]\n",
      "loss:    1.048290  [ 1500/20240]\n",
      "loss:    0.610839  [ 1600/20240]\n",
      "loss:    0.932066  [ 1700/20240]\n",
      "loss:    1.528949  [ 1800/20240]\n",
      "loss:    1.397347  [ 1900/20240]\n",
      "loss:    0.821682  [ 2000/20240]\n",
      "loss:    0.835136  [ 2100/20240]\n",
      "loss:    1.225990  [ 2200/20240]\n",
      "loss:    1.726125  [ 2300/20240]\n",
      "loss:    0.606776  [ 2400/20240]\n",
      "loss:    1.016057  [ 2500/20240]\n",
      "loss:    1.226789  [ 2600/20240]\n",
      "loss:    1.195956  [ 2700/20240]\n",
      "loss:    1.219326  [ 2800/20240]\n",
      "loss:    0.587655  [ 2900/20240]\n",
      "loss:    0.764855  [ 3000/20240]\n",
      "loss:    1.262525  [ 3100/20240]\n",
      "loss:    0.889447  [ 3200/20240]\n",
      "loss:    0.875748  [ 3300/20240]\n",
      "loss:    0.450503  [ 3400/20240]\n",
      "loss:    1.497522  [ 3500/20240]\n",
      "loss:    1.211634  [ 3600/20240]\n",
      "loss:    1.444814  [ 3700/20240]\n",
      "loss:    0.523609  [ 3800/20240]\n",
      "loss:    1.425707  [ 3900/20240]\n",
      "loss:    1.025168  [ 4000/20240]\n",
      "loss:    1.145504  [ 4100/20240]\n",
      "loss:    0.814740  [ 4200/20240]\n",
      "loss:    0.902247  [ 4300/20240]\n",
      "loss:    0.862851  [ 4400/20240]\n",
      "loss:    1.200567  [ 4500/20240]\n",
      "loss:    1.306747  [ 4600/20240]\n",
      "loss:    1.201832  [ 4700/20240]\n",
      "loss:    1.447710  [ 4800/20240]\n",
      "loss:    1.272492  [ 4900/20240]\n",
      "loss:    1.221006  [ 5000/20240]\n",
      "loss:    1.225387  [ 5100/20240]\n",
      "loss:    1.144922  [ 5200/20240]\n",
      "loss:    0.552747  [ 5300/20240]\n",
      "loss:    0.900326  [ 5400/20240]\n",
      "loss:    0.871503  [ 5500/20240]\n",
      "loss:    1.245710  [ 5600/20240]\n",
      "loss:    0.823676  [ 5700/20240]\n",
      "loss:    0.546943  [ 5800/20240]\n",
      "loss:    0.781610  [ 5900/20240]\n",
      "loss:    1.225616  [ 6000/20240]\n",
      "loss:    0.866140  [ 6100/20240]\n",
      "loss:    1.249232  [ 6200/20240]\n",
      "loss:    1.297147  [ 6300/20240]\n",
      "loss:    1.268924  [ 6400/20240]\n",
      "loss:    0.835491  [ 6500/20240]\n",
      "loss:    1.209065  [ 6600/20240]\n",
      "loss:    1.369653  [ 6700/20240]\n",
      "loss:    1.125724  [ 6800/20240]\n",
      "loss:    1.198215  [ 6900/20240]\n",
      "loss:    0.474929  [ 7000/20240]\n",
      "loss:    0.944882  [ 7100/20240]\n",
      "loss:    0.905901  [ 7200/20240]\n",
      "loss:    0.874136  [ 7300/20240]\n",
      "loss:    1.028755  [ 7400/20240]\n",
      "loss:    1.267375  [ 7500/20240]\n",
      "loss:    0.974277  [ 7600/20240]\n",
      "loss:    1.271893  [ 7700/20240]\n",
      "loss:    1.689185  [ 7800/20240]\n",
      "loss:    1.430166  [ 7900/20240]\n",
      "loss:    0.762785  [ 8000/20240]\n",
      "loss:    0.880835  [ 8100/20240]\n",
      "loss:    0.968401  [ 8200/20240]\n",
      "loss:    1.293149  [ 8300/20240]\n",
      "loss:    1.389022  [ 8400/20240]\n",
      "loss:    1.221335  [ 8500/20240]\n",
      "loss:    1.636118  [ 8600/20240]\n",
      "loss:    1.107005  [ 8700/20240]\n",
      "loss:    0.781041  [ 8800/20240]\n",
      "loss:    1.291165  [ 8900/20240]\n",
      "loss:    0.805022  [ 9000/20240]\n",
      "loss:    1.558704  [ 9100/20240]\n",
      "loss:    0.596349  [ 9200/20240]\n",
      "loss:    1.203825  [ 9300/20240]\n",
      "loss:    1.269180  [ 9400/20240]\n",
      "loss:    1.107206  [ 9500/20240]\n",
      "loss:    1.408952  [ 9600/20240]\n",
      "loss:    0.826380  [ 9700/20240]\n",
      "loss:    1.344851  [ 9800/20240]\n",
      "loss:    1.518806  [ 9900/20240]\n",
      "loss:    0.554750  [10000/20240]\n",
      "loss:    0.938460  [10100/20240]\n",
      "loss:    1.144783  [10200/20240]\n",
      "loss:    0.951644  [10300/20240]\n",
      "loss:    1.260133  [10400/20240]\n",
      "loss:    1.175518  [10500/20240]\n",
      "loss:    1.696539  [10600/20240]\n",
      "loss:    0.717517  [10700/20240]\n",
      "loss:    1.117911  [10800/20240]\n",
      "loss:    0.709673  [10900/20240]\n",
      "loss:    0.713364  [11000/20240]\n",
      "loss:    1.222505  [11100/20240]\n",
      "loss:    1.356508  [11200/20240]\n",
      "loss:    1.071005  [11300/20240]\n",
      "loss:    0.981062  [11400/20240]\n",
      "loss:    1.095993  [11500/20240]\n",
      "loss:    1.135566  [11600/20240]\n",
      "loss:    1.202374  [11700/20240]\n",
      "loss:    1.394211  [11800/20240]\n",
      "loss:    1.169161  [11900/20240]\n",
      "loss:    1.461909  [12000/20240]\n",
      "loss:    1.120207  [12100/20240]\n",
      "loss:    0.667674  [12200/20240]\n",
      "loss:    1.003379  [12300/20240]\n",
      "loss:    1.402780  [12400/20240]\n",
      "loss:    1.106930  [12500/20240]\n",
      "loss:    1.518846  [12600/20240]\n",
      "loss:    0.491506  [12700/20240]\n",
      "loss:    0.468519  [12800/20240]\n",
      "loss:    0.651212  [12900/20240]\n",
      "loss:    1.023688  [13000/20240]\n",
      "loss:    0.651724  [13100/20240]\n",
      "loss:    1.083926  [13200/20240]\n",
      "loss:    1.292442  [13300/20240]\n",
      "loss:    1.125359  [13400/20240]\n",
      "loss:    1.280574  [13500/20240]\n",
      "loss:    1.386898  [13600/20240]\n",
      "loss:    1.577293  [13700/20240]\n",
      "loss:    1.148810  [13800/20240]\n",
      "loss:    1.089837  [13900/20240]\n",
      "loss:    1.452734  [14000/20240]\n",
      "loss:    0.754290  [14100/20240]\n",
      "loss:    0.757020  [14200/20240]\n",
      "loss:    1.278436  [14300/20240]\n",
      "loss:    1.318040  [14400/20240]\n",
      "loss:    0.510517  [14500/20240]\n",
      "loss:    0.690433  [14600/20240]\n",
      "loss:    0.741113  [14700/20240]\n",
      "loss:    1.262019  [14800/20240]\n",
      "loss:    1.484431  [14900/20240]\n",
      "loss:    0.602614  [15000/20240]\n",
      "loss:    1.124785  [15100/20240]\n",
      "loss:    1.372492  [15200/20240]\n",
      "loss:    1.156620  [15300/20240]\n",
      "loss:    1.190430  [15400/20240]\n",
      "loss:    0.748196  [15500/20240]\n",
      "loss:    0.771876  [15600/20240]\n",
      "loss:    1.312659  [15700/20240]\n",
      "loss:    1.067612  [15800/20240]\n",
      "loss:    0.642804  [15900/20240]\n",
      "loss:    1.338438  [16000/20240]\n",
      "loss:    0.571456  [16100/20240]\n",
      "loss:    1.297811  [16200/20240]\n",
      "loss:    0.578240  [16300/20240]\n",
      "loss:    1.008027  [16400/20240]\n",
      "loss:    1.121368  [16500/20240]\n",
      "loss:    0.621334  [16600/20240]\n",
      "loss:    1.350081  [16700/20240]\n",
      "loss:    1.684233  [16800/20240]\n",
      "loss:    1.406364  [16900/20240]\n",
      "loss:    1.163965  [17000/20240]\n",
      "loss:    0.759485  [17100/20240]\n",
      "loss:    0.840268  [17200/20240]\n",
      "loss:    0.571498  [17300/20240]\n",
      "loss:    1.103021  [17400/20240]\n",
      "loss:    0.539699  [17500/20240]\n",
      "loss:    1.490258  [17600/20240]\n",
      "loss:    1.422313  [17700/20240]\n",
      "loss:    1.480512  [17800/20240]\n",
      "loss:    0.457034  [17900/20240]\n",
      "loss:    1.540723  [18000/20240]\n",
      "loss:    1.056891  [18100/20240]\n",
      "loss:    0.938285  [18200/20240]\n",
      "loss:    1.221424  [18300/20240]\n",
      "loss:    1.302790  [18400/20240]\n",
      "loss:    1.678771  [18500/20240]\n",
      "loss:    1.274661  [18600/20240]\n",
      "loss:    1.519890  [18700/20240]\n",
      "loss:    0.934961  [18800/20240]\n",
      "loss:    1.700255  [18900/20240]\n",
      "loss:    1.014572  [19000/20240]\n",
      "loss:    1.683548  [19100/20240]\n",
      "loss:    1.113624  [19200/20240]\n",
      "loss:    1.182351  [19300/20240]\n",
      "loss:    0.969215  [19400/20240]\n",
      "loss:    0.963232  [19500/20240]\n",
      "loss:    0.641174  [19600/20240]\n",
      "loss:    1.013145  [19700/20240]\n",
      "loss:    1.350137  [19800/20240]\n",
      "loss:    1.028748  [19900/20240]\n",
      "loss:    1.094843  [20000/20240]\n",
      "loss:    1.651558  [20100/20240]\n",
      "loss:    1.680552  [20200/20240]\n",
      "Test: \n",
      " Accuracy: 42.8%, Avg loss: 1.095019 \n",
      "\n",
      "======================================================\n",
      "epoch 2\n",
      "loss:    1.136326  [    0/20240]\n",
      "loss:    1.348192  [  100/20240]\n",
      "loss:    0.956036  [  200/20240]\n",
      "loss:    0.629367  [  300/20240]\n",
      "loss:    0.518903  [  400/20240]\n",
      "loss:    1.191992  [  500/20240]\n",
      "loss:    0.578193  [  600/20240]\n",
      "loss:    0.625568  [  700/20240]\n",
      "loss:    0.566474  [  800/20240]\n",
      "loss:    1.442446  [  900/20240]\n",
      "loss:    0.490389  [ 1000/20240]\n",
      "loss:    0.523557  [ 1100/20240]\n",
      "loss:    1.414194  [ 1200/20240]\n",
      "loss:    1.138242  [ 1300/20240]\n",
      "loss:    0.583718  [ 1400/20240]\n",
      "loss:    1.260483  [ 1500/20240]\n",
      "loss:    1.505939  [ 1600/20240]\n",
      "loss:    1.590605  [ 1700/20240]\n",
      "loss:    0.482459  [ 1800/20240]\n",
      "loss:    0.546859  [ 1900/20240]\n",
      "loss:    1.268831  [ 2000/20240]\n",
      "loss:    1.141008  [ 2100/20240]\n",
      "loss:    0.517024  [ 2200/20240]\n",
      "loss:    0.868124  [ 2300/20240]\n",
      "loss:    0.842031  [ 2400/20240]\n",
      "loss:    1.277558  [ 2500/20240]\n",
      "loss:    0.484142  [ 2600/20240]\n",
      "loss:    0.853611  [ 2700/20240]\n",
      "loss:    0.470344  [ 2800/20240]\n",
      "loss:    1.292375  [ 2900/20240]\n",
      "loss:    0.922653  [ 3000/20240]\n",
      "loss:    1.224980  [ 3100/20240]\n",
      "loss:    1.178581  [ 3200/20240]\n",
      "loss:    1.622342  [ 3300/20240]\n",
      "loss:    1.214432  [ 3400/20240]\n",
      "loss:    0.955779  [ 3500/20240]\n",
      "loss:    1.178940  [ 3600/20240]\n",
      "loss:    1.204220  [ 3700/20240]\n",
      "loss:    1.307937  [ 3800/20240]\n",
      "loss:    1.358093  [ 3900/20240]\n",
      "loss:    0.964435  [ 4000/20240]\n",
      "loss:    1.003648  [ 4100/20240]\n",
      "loss:    1.553711  [ 4200/20240]\n",
      "loss:    0.548993  [ 4300/20240]\n",
      "loss:    0.529563  [ 4400/20240]\n",
      "loss:    1.192392  [ 4500/20240]\n",
      "loss:    1.413076  [ 4600/20240]\n",
      "loss:    1.058207  [ 4700/20240]\n",
      "loss:    1.965585  [ 4800/20240]\n",
      "loss:    0.757685  [ 4900/20240]\n",
      "loss:    0.852200  [ 5000/20240]\n",
      "loss:    0.482804  [ 5100/20240]\n",
      "loss:    1.098470  [ 5200/20240]\n",
      "loss:    1.239159  [ 5300/20240]\n",
      "loss:    0.930657  [ 5400/20240]\n",
      "loss:    1.377811  [ 5500/20240]\n",
      "loss:    1.289385  [ 5600/20240]\n",
      "loss:    1.600136  [ 5700/20240]\n",
      "loss:    1.474832  [ 5800/20240]\n",
      "loss:    1.335641  [ 5900/20240]\n",
      "loss:    1.222104  [ 6000/20240]\n",
      "loss:    1.091192  [ 6100/20240]\n",
      "loss:    1.330339  [ 6200/20240]\n",
      "loss:    1.397519  [ 6300/20240]\n",
      "loss:    0.805033  [ 6400/20240]\n",
      "loss:    0.878409  [ 6500/20240]\n",
      "loss:    1.271487  [ 6600/20240]\n",
      "loss:    0.570010  [ 6700/20240]\n",
      "loss:    1.000205  [ 6800/20240]\n",
      "loss:    0.839048  [ 6900/20240]\n",
      "loss:    0.768591  [ 7000/20240]\n",
      "loss:    0.494471  [ 7100/20240]\n",
      "loss:    1.296085  [ 7200/20240]\n",
      "loss:    0.864125  [ 7300/20240]\n",
      "loss:    0.588425  [ 7400/20240]\n",
      "loss:    1.533388  [ 7500/20240]\n",
      "loss:    1.212900  [ 7600/20240]\n",
      "loss:    1.056405  [ 7700/20240]\n",
      "loss:    0.493993  [ 7800/20240]\n",
      "loss:    1.513458  [ 7900/20240]\n",
      "loss:    1.046226  [ 8000/20240]\n",
      "loss:    0.500336  [ 8100/20240]\n",
      "loss:    0.663116  [ 8200/20240]\n",
      "loss:    1.267617  [ 8300/20240]\n",
      "loss:    0.618992  [ 8400/20240]\n",
      "loss:    1.101470  [ 8500/20240]\n",
      "loss:    0.873204  [ 8600/20240]\n",
      "loss:    1.194207  [ 8700/20240]\n",
      "loss:    0.962768  [ 8800/20240]\n",
      "loss:    1.502293  [ 8900/20240]\n",
      "loss:    0.922883  [ 9000/20240]\n",
      "loss:    1.416164  [ 9100/20240]\n",
      "loss:    1.170766  [ 9200/20240]\n",
      "loss:    0.521379  [ 9300/20240]\n",
      "loss:    1.134002  [ 9400/20240]\n",
      "loss:    1.000251  [ 9500/20240]\n",
      "loss:    1.208886  [ 9600/20240]\n",
      "loss:    0.618996  [ 9700/20240]\n",
      "loss:    1.310189  [ 9800/20240]\n",
      "loss:    1.617030  [ 9900/20240]\n",
      "loss:    1.728011  [10000/20240]\n",
      "loss:    0.551495  [10100/20240]\n",
      "loss:    0.515861  [10200/20240]\n",
      "loss:    0.862373  [10300/20240]\n",
      "loss:    1.215492  [10400/20240]\n",
      "loss:    1.284560  [10500/20240]\n",
      "loss:    0.615756  [10600/20240]\n",
      "loss:    0.875638  [10700/20240]\n",
      "loss:    1.525152  [10800/20240]\n",
      "loss:    0.912760  [10900/20240]\n",
      "loss:    0.974568  [11000/20240]\n",
      "loss:    0.917299  [11100/20240]\n",
      "loss:    0.885574  [11200/20240]\n",
      "loss:    1.156730  [11300/20240]\n",
      "loss:    0.908263  [11400/20240]\n",
      "loss:    1.189734  [11500/20240]\n",
      "loss:    0.878084  [11600/20240]\n",
      "loss:    1.099882  [11700/20240]\n",
      "loss:    1.310688  [11800/20240]\n",
      "loss:    0.512798  [11900/20240]\n",
      "loss:    1.175140  [12000/20240]\n",
      "loss:    0.527707  [12100/20240]\n",
      "loss:    0.663606  [12200/20240]\n",
      "loss:    0.993984  [12300/20240]\n",
      "loss:    1.374565  [12400/20240]\n",
      "loss:    0.382632  [12500/20240]\n",
      "loss:    0.986226  [12600/20240]\n",
      "loss:    0.906831  [12700/20240]\n",
      "loss:    1.252275  [12800/20240]\n",
      "loss:    0.562731  [12900/20240]\n",
      "loss:    0.972326  [13000/20240]\n",
      "loss:    1.572992  [13100/20240]\n",
      "loss:    1.066293  [13200/20240]\n",
      "loss:    1.111658  [13300/20240]\n",
      "loss:    1.060947  [13400/20240]\n",
      "loss:    1.167980  [13500/20240]\n",
      "loss:    1.260858  [13600/20240]\n",
      "loss:    0.364805  [13700/20240]\n",
      "loss:    0.458931  [13800/20240]\n",
      "loss:    1.246775  [13900/20240]\n",
      "loss:    1.009281  [14000/20240]\n",
      "loss:    1.265525  [14100/20240]\n",
      "loss:    0.459463  [14200/20240]\n",
      "loss:    1.001838  [14300/20240]\n",
      "loss:    0.659134  [14400/20240]\n",
      "loss:    1.341320  [14500/20240]\n",
      "loss:    0.556554  [14600/20240]\n",
      "loss:    0.607383  [14700/20240]\n",
      "loss:    1.020627  [14800/20240]\n",
      "loss:    1.263611  [14900/20240]\n",
      "loss:    1.547232  [15000/20240]\n",
      "loss:    0.721884  [15100/20240]\n",
      "loss:    1.187095  [15200/20240]\n",
      "loss:    0.744916  [15300/20240]\n",
      "loss:    0.724833  [15400/20240]\n",
      "loss:    0.958621  [15500/20240]\n",
      "loss:    0.528834  [15600/20240]\n",
      "loss:    1.591179  [15700/20240]\n",
      "loss:    0.958160  [15800/20240]\n",
      "loss:    0.619476  [15900/20240]\n",
      "loss:    1.354798  [16000/20240]\n",
      "loss:    0.808915  [16100/20240]\n",
      "loss:    1.648352  [16200/20240]\n",
      "loss:    1.025603  [16300/20240]\n",
      "loss:    1.141214  [16400/20240]\n",
      "loss:    1.073523  [16500/20240]\n",
      "loss:    1.138915  [16600/20240]\n",
      "loss:    0.843521  [16700/20240]\n",
      "loss:    1.765302  [16800/20240]\n",
      "loss:    1.634268  [16900/20240]\n",
      "loss:    1.030479  [17000/20240]\n",
      "loss:    0.882867  [17100/20240]\n",
      "loss:    0.964097  [17200/20240]\n",
      "loss:    0.765212  [17300/20240]\n",
      "loss:    1.697577  [17400/20240]\n",
      "loss:    1.107960  [17500/20240]\n",
      "loss:    0.928395  [17600/20240]\n",
      "loss:    1.114682  [17700/20240]\n",
      "loss:    0.363537  [17800/20240]\n",
      "loss:    0.475605  [17900/20240]\n",
      "loss:    1.226844  [18000/20240]\n",
      "loss:    0.792632  [18100/20240]\n",
      "loss:    0.670695  [18200/20240]\n",
      "loss:    1.266961  [18300/20240]\n",
      "loss:    1.372123  [18400/20240]\n",
      "loss:    0.830256  [18500/20240]\n",
      "loss:    0.755664  [18600/20240]\n",
      "loss:    1.273818  [18700/20240]\n",
      "loss:    1.011529  [18800/20240]\n",
      "loss:    1.747056  [18900/20240]\n",
      "loss:    0.754785  [19000/20240]\n",
      "loss:    0.508216  [19100/20240]\n",
      "loss:    0.438083  [19200/20240]\n",
      "loss:    0.683750  [19300/20240]\n",
      "loss:    1.322372  [19400/20240]\n",
      "loss:    0.697912  [19500/20240]\n",
      "loss:    0.444429  [19600/20240]\n",
      "loss:    1.114189  [19700/20240]\n",
      "loss:    1.402989  [19800/20240]\n",
      "loss:    0.791750  [19900/20240]\n",
      "loss:    1.368617  [20000/20240]\n",
      "loss:    0.896254  [20100/20240]\n",
      "loss:    1.095018  [20200/20240]\n",
      "Test: \n",
      " Accuracy: 42.2%, Avg loss: 1.087043 \n",
      "\n",
      "======================================================\n",
      "epoch 3\n",
      "loss:    1.138304  [    0/20240]\n",
      "loss:    0.901740  [  100/20240]\n",
      "loss:    1.022139  [  200/20240]\n",
      "loss:    0.914238  [  300/20240]\n",
      "loss:    0.583799  [  400/20240]\n",
      "loss:    1.259487  [  500/20240]\n",
      "loss:    1.075697  [  600/20240]\n",
      "loss:    0.787403  [  700/20240]\n",
      "loss:    0.405556  [  800/20240]\n",
      "loss:    0.458860  [  900/20240]\n",
      "loss:    0.567174  [ 1000/20240]\n",
      "loss:    1.503157  [ 1100/20240]\n",
      "loss:    0.755775  [ 1200/20240]\n",
      "loss:    1.169815  [ 1300/20240]\n",
      "loss:    1.534577  [ 1400/20240]\n",
      "loss:    0.386622  [ 1500/20240]\n",
      "loss:    1.663121  [ 1600/20240]\n",
      "loss:    0.369720  [ 1700/20240]\n",
      "loss:    0.710902  [ 1800/20240]\n",
      "loss:    1.311263  [ 1900/20240]\n",
      "loss:    0.717829  [ 2000/20240]\n",
      "loss:    1.554255  [ 2100/20240]\n",
      "loss:    1.311903  [ 2200/20240]\n",
      "loss:    1.626723  [ 2300/20240]\n",
      "loss:    1.621484  [ 2400/20240]\n",
      "loss:    0.772036  [ 2500/20240]\n",
      "loss:    1.498671  [ 2600/20240]\n",
      "loss:    0.853092  [ 2700/20240]\n",
      "loss:    0.697681  [ 2800/20240]\n",
      "loss:    0.708940  [ 2900/20240]\n",
      "loss:    1.286465  [ 3000/20240]\n",
      "loss:    0.769644  [ 3100/20240]\n",
      "loss:    0.667168  [ 3200/20240]\n",
      "loss:    0.949417  [ 3300/20240]\n",
      "loss:    0.475941  [ 3400/20240]\n",
      "loss:    1.000511  [ 3500/20240]\n",
      "loss:    0.648440  [ 3600/20240]\n",
      "loss:    0.451477  [ 3700/20240]\n",
      "loss:    0.849602  [ 3800/20240]\n",
      "loss:    1.284386  [ 3900/20240]\n",
      "loss:    1.355231  [ 4000/20240]\n",
      "loss:    1.804308  [ 4100/20240]\n",
      "loss:    0.881985  [ 4200/20240]\n",
      "loss:    0.963205  [ 4300/20240]\n",
      "loss:    0.878291  [ 4400/20240]\n",
      "loss:    1.782835  [ 4500/20240]\n",
      "loss:    0.564115  [ 4600/20240]\n",
      "loss:    0.910132  [ 4700/20240]\n",
      "loss:    1.473775  [ 4800/20240]\n",
      "loss:    0.739163  [ 4900/20240]\n",
      "loss:    0.463452  [ 5000/20240]\n",
      "loss:    0.364958  [ 5100/20240]\n",
      "loss:    1.597858  [ 5200/20240]\n",
      "loss:    1.234960  [ 5300/20240]\n",
      "loss:    1.284451  [ 5400/20240]\n",
      "loss:    0.697814  [ 5500/20240]\n",
      "loss:    0.371777  [ 5600/20240]\n",
      "loss:    1.834502  [ 5700/20240]\n",
      "loss:    1.717179  [ 5800/20240]\n",
      "loss:    1.446152  [ 5900/20240]\n",
      "loss:    0.617191  [ 6000/20240]\n",
      "loss:    1.472225  [ 6100/20240]\n",
      "loss:    1.252942  [ 6200/20240]\n",
      "loss:    1.257992  [ 6300/20240]\n",
      "loss:    1.332595  [ 6400/20240]\n",
      "loss:    1.339711  [ 6500/20240]\n",
      "loss:    0.601413  [ 6600/20240]\n",
      "loss:    0.715531  [ 6700/20240]\n",
      "loss:    1.454842  [ 6800/20240]\n",
      "loss:    1.244026  [ 6900/20240]\n",
      "loss:    1.213268  [ 7000/20240]\n",
      "loss:    0.677290  [ 7100/20240]\n",
      "loss:    0.400283  [ 7200/20240]\n",
      "loss:    0.722489  [ 7300/20240]\n",
      "loss:    0.739576  [ 7400/20240]\n",
      "loss:    1.023896  [ 7500/20240]\n",
      "loss:    0.736343  [ 7600/20240]\n",
      "loss:    1.671522  [ 7700/20240]\n",
      "loss:    1.055949  [ 7800/20240]\n",
      "loss:    1.238700  [ 7900/20240]\n",
      "loss:    0.738481  [ 8000/20240]\n",
      "loss:    1.050580  [ 8100/20240]\n",
      "loss:    1.807067  [ 8200/20240]\n",
      "loss:    1.733328  [ 8300/20240]\n",
      "loss:    1.036922  [ 8400/20240]\n",
      "loss:    0.855789  [ 8500/20240]\n",
      "loss:    0.392485  [ 8600/20240]\n",
      "loss:    0.563995  [ 8700/20240]\n",
      "loss:    0.481791  [ 8800/20240]\n",
      "loss:    0.581944  [ 8900/20240]\n",
      "loss:    1.253911  [ 9000/20240]\n",
      "loss:    1.005058  [ 9100/20240]\n",
      "loss:    1.431150  [ 9200/20240]\n",
      "loss:    0.462239  [ 9300/20240]\n",
      "loss:    1.112235  [ 9400/20240]\n",
      "loss:    0.984349  [ 9500/20240]\n",
      "loss:    0.495611  [ 9600/20240]\n",
      "loss:    0.993850  [ 9700/20240]\n",
      "loss:    0.464134  [ 9800/20240]\n",
      "loss:    1.521454  [ 9900/20240]\n",
      "loss:    0.965579  [10000/20240]\n",
      "loss:    0.689015  [10100/20240]\n",
      "loss:    0.788071  [10200/20240]\n",
      "loss:    1.392902  [10300/20240]\n",
      "loss:    1.421225  [10400/20240]\n",
      "loss:    0.638190  [10500/20240]\n",
      "loss:    0.810359  [10600/20240]\n",
      "loss:    1.002936  [10700/20240]\n",
      "loss:    1.788193  [10800/20240]\n",
      "loss:    0.391049  [10900/20240]\n",
      "loss:    0.654436  [11000/20240]\n",
      "loss:    0.647552  [11100/20240]\n",
      "loss:    0.514934  [11200/20240]\n",
      "loss:    1.345632  [11300/20240]\n",
      "loss:    0.926796  [11400/20240]\n",
      "loss:    1.684109  [11500/20240]\n",
      "loss:    1.380959  [11600/20240]\n",
      "loss:    0.930694  [11700/20240]\n",
      "loss:    0.517122  [11800/20240]\n",
      "loss:    0.925092  [11900/20240]\n",
      "loss:    1.448284  [12000/20240]\n",
      "loss:    1.144875  [12100/20240]\n",
      "loss:    1.553251  [12200/20240]\n",
      "loss:    1.186939  [12300/20240]\n",
      "loss:    0.721538  [12400/20240]\n",
      "loss:    1.183525  [12500/20240]\n",
      "loss:    1.721182  [12600/20240]\n",
      "loss:    1.383546  [12700/20240]\n",
      "loss:    0.680824  [12800/20240]\n",
      "loss:    1.093531  [12900/20240]\n",
      "loss:    1.809487  [13000/20240]\n",
      "loss:    1.625831  [13100/20240]\n",
      "loss:    0.432153  [13200/20240]\n",
      "loss:    0.456142  [13300/20240]\n",
      "loss:    1.162409  [13400/20240]\n",
      "loss:    1.341334  [13500/20240]\n",
      "loss:    0.398495  [13600/20240]\n",
      "loss:    0.597159  [13700/20240]\n",
      "loss:    0.325221  [13800/20240]\n",
      "loss:    1.331334  [13900/20240]\n",
      "loss:    0.975212  [14000/20240]\n",
      "loss:    1.414275  [14100/20240]\n",
      "loss:    0.531275  [14200/20240]\n",
      "loss:    0.644234  [14300/20240]\n",
      "loss:    0.662896  [14400/20240]\n",
      "loss:    0.877727  [14500/20240]\n",
      "loss:    1.292762  [14600/20240]\n",
      "loss:    0.961360  [14700/20240]\n",
      "loss:    1.074866  [14800/20240]\n",
      "loss:    0.774596  [14900/20240]\n",
      "loss:    1.080941  [15000/20240]\n",
      "loss:    0.809709  [15100/20240]\n",
      "loss:    0.394205  [15200/20240]\n",
      "loss:    1.288216  [15300/20240]\n",
      "loss:    0.706367  [15400/20240]\n",
      "loss:    0.855624  [15500/20240]\n",
      "loss:    0.286997  [15600/20240]\n",
      "loss:    0.619619  [15700/20240]\n",
      "loss:    0.713176  [15800/20240]\n",
      "loss:    1.606256  [15900/20240]\n",
      "loss:    1.773892  [16000/20240]\n",
      "loss:    1.243321  [16100/20240]\n",
      "loss:    0.552268  [16200/20240]\n",
      "loss:    1.548210  [16300/20240]\n",
      "loss:    1.230489  [16400/20240]\n",
      "loss:    1.029606  [16500/20240]\n",
      "loss:    1.062335  [16600/20240]\n",
      "loss:    1.490775  [16700/20240]\n",
      "loss:    1.808770  [16800/20240]\n",
      "loss:    0.710127  [16900/20240]\n",
      "loss:    1.165591  [17000/20240]\n",
      "loss:    0.865793  [17100/20240]\n",
      "loss:    1.358700  [17200/20240]\n",
      "loss:    1.705572  [17300/20240]\n",
      "loss:    0.891519  [17400/20240]\n",
      "loss:    0.896629  [17500/20240]\n",
      "loss:    0.747326  [17600/20240]\n",
      "loss:    1.344369  [17700/20240]\n",
      "loss:    0.935250  [17800/20240]\n",
      "loss:    1.656572  [17900/20240]\n",
      "loss:    0.786754  [18000/20240]\n",
      "loss:    0.707782  [18100/20240]\n",
      "loss:    1.448659  [18200/20240]\n",
      "loss:    0.796662  [18300/20240]\n",
      "loss:    0.654458  [18400/20240]\n",
      "loss:    1.070322  [18500/20240]\n",
      "loss:    1.353333  [18600/20240]\n",
      "loss:    1.276191  [18700/20240]\n",
      "loss:    0.616758  [18800/20240]\n",
      "loss:    0.855225  [18900/20240]\n",
      "loss:    1.686219  [19000/20240]\n",
      "loss:    1.272946  [19100/20240]\n",
      "loss:    0.596244  [19200/20240]\n",
      "loss:    0.443794  [19300/20240]\n",
      "loss:    0.893260  [19400/20240]\n",
      "loss:    1.262454  [19500/20240]\n",
      "loss:    0.311448  [19600/20240]\n",
      "loss:    0.917114  [19700/20240]\n",
      "loss:    0.470837  [19800/20240]\n",
      "loss:    1.152069  [19900/20240]\n",
      "loss:    1.122860  [20000/20240]\n",
      "loss:    0.785005  [20100/20240]\n",
      "loss:    1.457381  [20200/20240]\n",
      "Test: \n",
      " Accuracy: 46.0%, Avg loss: 1.060835 \n",
      "\n",
      "======================================================\n",
      "epoch 4\n",
      "loss:    0.643039  [    0/20240]\n",
      "loss:    1.403549  [  100/20240]\n",
      "loss:    1.558971  [  200/20240]\n",
      "loss:    1.780456  [  300/20240]\n",
      "loss:    0.911802  [  400/20240]\n",
      "loss:    0.796397  [  500/20240]\n",
      "loss:    0.774665  [  600/20240]\n",
      "loss:    1.093243  [  700/20240]\n",
      "loss:    0.811984  [  800/20240]\n",
      "loss:    1.754152  [  900/20240]\n",
      "loss:    0.338523  [ 1000/20240]\n",
      "loss:    0.840115  [ 1100/20240]\n",
      "loss:    1.368203  [ 1200/20240]\n",
      "loss:    0.934573  [ 1300/20240]\n",
      "loss:    0.396084  [ 1400/20240]\n",
      "loss:    1.421597  [ 1500/20240]\n",
      "loss:    2.051260  [ 1600/20240]\n",
      "loss:    1.204471  [ 1700/20240]\n",
      "loss:    1.482663  [ 1800/20240]\n",
      "loss:    0.674240  [ 1900/20240]\n",
      "loss:    0.818924  [ 2000/20240]\n",
      "loss:    0.627359  [ 2100/20240]\n",
      "loss:    1.166300  [ 2200/20240]\n",
      "loss:    1.162143  [ 2300/20240]\n",
      "loss:    1.610875  [ 2400/20240]\n",
      "loss:    1.195333  [ 2500/20240]\n",
      "loss:    0.778998  [ 2600/20240]\n",
      "loss:    1.003211  [ 2700/20240]\n",
      "loss:    1.179804  [ 2800/20240]\n",
      "loss:    0.647638  [ 2900/20240]\n",
      "loss:    1.094671  [ 3000/20240]\n",
      "loss:    1.251917  [ 3100/20240]\n",
      "loss:    1.362306  [ 3200/20240]\n",
      "loss:    0.739746  [ 3300/20240]\n",
      "loss:    0.399266  [ 3400/20240]\n",
      "loss:    0.928842  [ 3500/20240]\n",
      "loss:    0.501428  [ 3600/20240]\n",
      "loss:    0.925182  [ 3700/20240]\n",
      "loss:    1.656666  [ 3800/20240]\n",
      "loss:    0.339631  [ 3900/20240]\n",
      "loss:    0.749635  [ 4000/20240]\n",
      "loss:    1.456355  [ 4100/20240]\n",
      "loss:    1.511814  [ 4200/20240]\n",
      "loss:    1.298338  [ 4300/20240]\n",
      "loss:    1.609810  [ 4400/20240]\n",
      "loss:    1.680765  [ 4500/20240]\n",
      "loss:    0.639802  [ 4600/20240]\n",
      "loss:    1.087544  [ 4700/20240]\n",
      "loss:    1.370111  [ 4800/20240]\n",
      "loss:    1.029953  [ 4900/20240]\n",
      "loss:    0.590187  [ 5000/20240]\n",
      "loss:    0.421743  [ 5100/20240]\n",
      "loss:    0.809513  [ 5200/20240]\n",
      "loss:    0.531134  [ 5300/20240]\n",
      "loss:    0.942537  [ 5400/20240]\n",
      "loss:    0.614834  [ 5500/20240]\n",
      "loss:    1.537516  [ 5600/20240]\n",
      "loss:    1.164584  [ 5700/20240]\n",
      "loss:    1.194842  [ 5800/20240]\n",
      "loss:    1.205899  [ 5900/20240]\n",
      "loss:    0.531046  [ 6000/20240]\n",
      "loss:    1.564412  [ 6100/20240]\n",
      "loss:    1.384485  [ 6200/20240]\n",
      "loss:    0.812911  [ 6300/20240]\n",
      "loss:    1.883068  [ 6400/20240]\n",
      "loss:    0.928181  [ 6500/20240]\n",
      "loss:    0.820139  [ 6600/20240]\n",
      "loss:    0.527702  [ 6700/20240]\n",
      "loss:    0.980743  [ 6800/20240]\n",
      "loss:    0.974041  [ 6900/20240]\n",
      "loss:    1.214618  [ 7000/20240]\n",
      "loss:    1.123924  [ 7100/20240]\n",
      "loss:    0.908351  [ 7200/20240]\n",
      "loss:    0.413128  [ 7300/20240]\n",
      "loss:    0.477041  [ 7400/20240]\n",
      "loss:    1.273611  [ 7500/20240]\n",
      "loss:    0.581156  [ 7600/20240]\n",
      "loss:    0.717976  [ 7700/20240]\n",
      "loss:    0.463850  [ 7800/20240]\n",
      "loss:    0.329239  [ 7900/20240]\n",
      "loss:    0.439802  [ 8000/20240]\n",
      "loss:    1.501464  [ 8100/20240]\n",
      "loss:    2.036392  [ 8200/20240]\n",
      "loss:    1.856764  [ 8300/20240]\n",
      "loss:    1.520729  [ 8400/20240]\n",
      "loss:    1.333764  [ 8500/20240]\n",
      "loss:    1.200182  [ 8600/20240]\n",
      "loss:    0.715479  [ 8700/20240]\n",
      "loss:    1.195790  [ 8800/20240]\n",
      "loss:    0.771992  [ 8900/20240]\n",
      "loss:    0.462734  [ 9000/20240]\n",
      "loss:    1.185984  [ 9100/20240]\n",
      "loss:    1.367753  [ 9200/20240]\n",
      "loss:    0.552961  [ 9300/20240]\n",
      "loss:    1.356905  [ 9400/20240]\n",
      "loss:    1.054016  [ 9500/20240]\n",
      "loss:    1.031977  [ 9600/20240]\n",
      "loss:    0.946866  [ 9700/20240]\n",
      "loss:    1.124905  [ 9800/20240]\n",
      "loss:    0.980614  [ 9900/20240]\n",
      "loss:    1.391768  [10000/20240]\n",
      "loss:    0.778592  [10100/20240]\n",
      "loss:    0.786547  [10200/20240]\n",
      "loss:    1.226429  [10300/20240]\n",
      "loss:    0.625396  [10400/20240]\n",
      "loss:    1.665221  [10500/20240]\n",
      "loss:    1.429057  [10600/20240]\n",
      "loss:    1.335351  [10700/20240]\n",
      "loss:    0.763932  [10800/20240]\n",
      "loss:    0.608952  [10900/20240]\n",
      "loss:    0.328822  [11000/20240]\n",
      "loss:    0.806561  [11100/20240]\n",
      "loss:    0.946677  [11200/20240]\n",
      "loss:    0.585896  [11300/20240]\n",
      "loss:    0.844876  [11400/20240]\n",
      "loss:    0.994783  [11500/20240]\n",
      "loss:    0.608037  [11600/20240]\n",
      "loss:    0.795401  [11700/20240]\n",
      "loss:    0.951977  [11800/20240]\n",
      "loss:    1.651106  [11900/20240]\n",
      "loss:    1.084486  [12000/20240]\n",
      "loss:    1.199995  [12100/20240]\n",
      "loss:    0.798722  [12200/20240]\n",
      "loss:    0.884276  [12300/20240]\n",
      "loss:    0.830574  [12400/20240]\n",
      "loss:    1.306251  [12500/20240]\n",
      "loss:    1.086446  [12600/20240]\n",
      "loss:    1.060361  [12700/20240]\n",
      "loss:    1.722423  [12800/20240]\n",
      "loss:    1.085417  [12900/20240]\n",
      "loss:    0.429625  [13000/20240]\n",
      "loss:    0.771682  [13100/20240]\n",
      "loss:    0.302435  [13200/20240]\n",
      "loss:    1.172692  [13300/20240]\n",
      "loss:    0.659728  [13400/20240]\n",
      "loss:    0.349693  [13500/20240]\n",
      "loss:    1.302512  [13600/20240]\n",
      "loss:    0.388015  [13700/20240]\n",
      "loss:    1.852934  [13800/20240]\n",
      "loss:    1.233016  [13900/20240]\n",
      "loss:    0.348175  [14000/20240]\n",
      "loss:    1.172003  [14100/20240]\n",
      "loss:    0.698778  [14200/20240]\n",
      "loss:    0.363514  [14300/20240]\n",
      "loss:    0.532211  [14400/20240]\n",
      "loss:    0.441148  [14500/20240]\n",
      "loss:    0.326545  [14600/20240]\n",
      "loss:    0.380374  [14700/20240]\n",
      "loss:    0.777625  [14800/20240]\n",
      "loss:    1.032861  [14900/20240]\n",
      "loss:    1.404908  [15000/20240]\n",
      "loss:    0.812621  [15100/20240]\n",
      "loss:    1.327276  [15200/20240]\n",
      "loss:    1.024407  [15300/20240]\n",
      "loss:    0.841479  [15400/20240]\n",
      "loss:    0.986936  [15500/20240]\n",
      "loss:    0.628577  [15600/20240]\n",
      "loss:    0.902064  [15700/20240]\n",
      "loss:    0.908665  [15800/20240]\n",
      "loss:    1.094748  [15900/20240]\n",
      "loss:    0.758020  [16000/20240]\n",
      "loss:    1.385702  [16100/20240]\n",
      "loss:    2.003417  [16200/20240]\n",
      "loss:    1.087566  [16300/20240]\n",
      "loss:    1.227814  [16400/20240]\n",
      "loss:    1.177499  [16500/20240]\n",
      "loss:    1.922108  [16600/20240]\n",
      "loss:    0.497403  [16700/20240]\n",
      "loss:    1.199860  [16800/20240]\n",
      "loss:    1.136234  [16900/20240]\n",
      "loss:    1.584594  [17000/20240]\n",
      "loss:    1.624316  [17100/20240]\n",
      "loss:    0.591329  [17200/20240]\n",
      "loss:    0.645996  [17300/20240]\n",
      "loss:    0.308349  [17400/20240]\n",
      "loss:    1.772457  [17500/20240]\n",
      "loss:    0.322480  [17600/20240]\n",
      "loss:    1.141596  [17700/20240]\n",
      "loss:    0.553748  [17800/20240]\n",
      "loss:    0.518193  [17900/20240]\n",
      "loss:    0.972819  [18000/20240]\n",
      "loss:    0.879963  [18100/20240]\n",
      "loss:    0.669219  [18200/20240]\n",
      "loss:    0.631118  [18300/20240]\n",
      "loss:    0.532135  [18400/20240]\n",
      "loss:    0.624127  [18500/20240]\n",
      "loss:    0.381644  [18600/20240]\n",
      "loss:    0.848544  [18700/20240]\n",
      "loss:    1.733466  [18800/20240]\n",
      "loss:    0.868136  [18900/20240]\n",
      "loss:    0.369700  [19000/20240]\n",
      "loss:    1.226563  [19100/20240]\n",
      "loss:    1.285301  [19200/20240]\n",
      "loss:    0.737699  [19300/20240]\n",
      "loss:    0.522063  [19400/20240]\n",
      "loss:    0.923369  [19500/20240]\n",
      "loss:    0.825328  [19600/20240]\n",
      "loss:    1.563963  [19700/20240]\n",
      "loss:    1.366942  [19800/20240]\n",
      "loss:    1.112960  [19900/20240]\n",
      "loss:    0.434798  [20000/20240]\n",
      "loss:    1.949371  [20100/20240]\n",
      "loss:    1.241281  [20200/20240]\n",
      "Test: \n",
      " Accuracy: 43.0%, Avg loss: 1.091505 \n",
      "\n",
      "======================================================\n",
      "epoch 5\n",
      "loss:    0.782055  [    0/20240]\n",
      "loss:    0.804923  [  100/20240]\n",
      "loss:    0.648917  [  200/20240]\n",
      "loss:    1.587147  [  300/20240]\n",
      "loss:    1.334490  [  400/20240]\n",
      "loss:    0.443305  [  500/20240]\n",
      "loss:    1.261092  [  600/20240]\n",
      "loss:    0.828424  [  700/20240]\n",
      "loss:    0.533100  [  800/20240]\n",
      "loss:    0.823959  [  900/20240]\n",
      "loss:    0.477213  [ 1000/20240]\n",
      "loss:    1.147591  [ 1100/20240]\n",
      "loss:    0.495329  [ 1200/20240]\n",
      "loss:    1.907168  [ 1300/20240]\n",
      "loss:    0.362143  [ 1400/20240]\n",
      "loss:    1.083431  [ 1500/20240]\n",
      "loss:    0.960790  [ 1600/20240]\n",
      "loss:    0.301593  [ 1700/20240]\n",
      "loss:    1.645836  [ 1800/20240]\n",
      "loss:    0.528879  [ 1900/20240]\n",
      "loss:    1.302163  [ 2000/20240]\n",
      "loss:    1.342752  [ 2100/20240]\n",
      "loss:    1.335371  [ 2200/20240]\n",
      "loss:    0.343485  [ 2300/20240]\n",
      "loss:    1.084897  [ 2400/20240]\n",
      "loss:    0.431641  [ 2500/20240]\n",
      "loss:    0.572851  [ 2600/20240]\n",
      "loss:    0.426424  [ 2700/20240]\n",
      "loss:    0.842911  [ 2800/20240]\n",
      "loss:    1.004036  [ 2900/20240]\n",
      "loss:    1.319383  [ 3000/20240]\n",
      "loss:    1.656271  [ 3100/20240]\n",
      "loss:    0.702268  [ 3200/20240]\n",
      "loss:    0.743382  [ 3300/20240]\n",
      "loss:    1.145955  [ 3400/20240]\n",
      "loss:    1.172420  [ 3500/20240]\n",
      "loss:    1.023278  [ 3600/20240]\n",
      "loss:    0.725397  [ 3700/20240]\n",
      "loss:    1.128587  [ 3800/20240]\n",
      "loss:    1.266967  [ 3900/20240]\n",
      "loss:    0.744983  [ 4000/20240]\n",
      "loss:    0.694500  [ 4100/20240]\n",
      "loss:    1.925454  [ 4200/20240]\n",
      "loss:    1.888570  [ 4300/20240]\n",
      "loss:    0.818738  [ 4400/20240]\n",
      "loss:    1.108695  [ 4500/20240]\n",
      "loss:    1.245645  [ 4600/20240]\n",
      "loss:    1.197436  [ 4700/20240]\n",
      "loss:    1.090615  [ 4800/20240]\n",
      "loss:    0.498164  [ 4900/20240]\n",
      "loss:    0.898071  [ 5000/20240]\n",
      "loss:    0.340847  [ 5100/20240]\n",
      "loss:    1.587628  [ 5200/20240]\n",
      "loss:    1.034469  [ 5300/20240]\n",
      "loss:    0.559738  [ 5400/20240]\n",
      "loss:    1.238302  [ 5500/20240]\n",
      "loss:    1.316699  [ 5600/20240]\n",
      "loss:    0.836085  [ 5700/20240]\n",
      "loss:    1.613851  [ 5800/20240]\n",
      "loss:    1.276093  [ 5900/20240]\n",
      "loss:    2.102388  [ 6000/20240]\n",
      "loss:    1.398621  [ 6100/20240]\n",
      "loss:    0.530145  [ 6200/20240]\n",
      "loss:    0.646089  [ 6300/20240]\n",
      "loss:    0.644213  [ 6400/20240]\n",
      "loss:    1.205201  [ 6500/20240]\n",
      "loss:    0.682382  [ 6600/20240]\n",
      "loss:    0.479682  [ 6700/20240]\n",
      "loss:    0.478438  [ 6800/20240]\n",
      "loss:    0.948509  [ 6900/20240]\n",
      "loss:    1.370056  [ 7000/20240]\n",
      "loss:    1.172327  [ 7100/20240]\n",
      "loss:    2.233491  [ 7200/20240]\n",
      "loss:    1.442318  [ 7300/20240]\n",
      "loss:    1.120339  [ 7400/20240]\n",
      "loss:    1.458043  [ 7500/20240]\n",
      "loss:    0.987276  [ 7600/20240]\n",
      "loss:    1.176412  [ 7700/20240]\n",
      "loss:    0.995144  [ 7800/20240]\n",
      "loss:    0.810874  [ 7900/20240]\n",
      "loss:    0.876006  [ 8000/20240]\n",
      "loss:    1.045545  [ 8100/20240]\n",
      "loss:    0.361279  [ 8200/20240]\n",
      "loss:    1.559894  [ 8300/20240]\n",
      "loss:    0.793217  [ 8400/20240]\n",
      "loss:    0.862171  [ 8500/20240]\n",
      "loss:    0.407778  [ 8600/20240]\n",
      "loss:    0.880743  [ 8700/20240]\n",
      "loss:    1.715620  [ 8800/20240]\n",
      "loss:    1.382015  [ 8900/20240]\n",
      "loss:    1.345255  [ 9000/20240]\n",
      "loss:    0.891801  [ 9100/20240]\n",
      "loss:    0.951845  [ 9200/20240]\n",
      "loss:    1.107116  [ 9300/20240]\n",
      "loss:    0.478441  [ 9400/20240]\n",
      "loss:    1.471684  [ 9500/20240]\n",
      "loss:    1.157167  [ 9600/20240]\n",
      "loss:    0.932114  [ 9700/20240]\n",
      "loss:    0.700649  [ 9800/20240]\n",
      "loss:    0.404562  [ 9900/20240]\n",
      "loss:    1.698873  [10000/20240]\n",
      "loss:    0.940765  [10100/20240]\n",
      "loss:    0.559994  [10200/20240]\n",
      "loss:    1.127160  [10300/20240]\n",
      "loss:    1.241973  [10400/20240]\n",
      "loss:    1.282775  [10500/20240]\n",
      "loss:    0.542121  [10600/20240]\n",
      "loss:    0.700605  [10700/20240]\n",
      "loss:    1.641348  [10800/20240]\n",
      "loss:    1.173353  [10900/20240]\n",
      "loss:    0.909083  [11000/20240]\n",
      "loss:    1.015589  [11100/20240]\n",
      "loss:    0.824735  [11200/20240]\n",
      "loss:    1.647189  [11300/20240]\n",
      "loss:    0.900229  [11400/20240]\n",
      "loss:    0.334047  [11500/20240]\n",
      "loss:    0.632291  [11600/20240]\n",
      "loss:    0.351417  [11700/20240]\n",
      "loss:    1.849163  [11800/20240]\n",
      "loss:    0.361306  [11900/20240]\n",
      "loss:    1.751494  [12000/20240]\n",
      "loss:    1.284163  [12100/20240]\n",
      "loss:    1.184338  [12200/20240]\n",
      "loss:    0.330803  [12300/20240]\n",
      "loss:    2.014674  [12400/20240]\n",
      "loss:    1.106029  [12500/20240]\n",
      "loss:    1.027419  [12600/20240]\n",
      "loss:    0.884009  [12700/20240]\n",
      "loss:    0.677801  [12800/20240]\n",
      "loss:    1.069140  [12900/20240]\n",
      "loss:    0.696530  [13000/20240]\n",
      "loss:    0.707484  [13100/20240]\n",
      "loss:    0.365188  [13200/20240]\n",
      "loss:    0.793324  [13300/20240]\n",
      "loss:    0.553002  [13400/20240]\n",
      "loss:    1.167201  [13500/20240]\n",
      "loss:    0.440422  [13600/20240]\n",
      "loss:    1.516265  [13700/20240]\n",
      "loss:    1.089347  [13800/20240]\n",
      "loss:    0.809636  [13900/20240]\n",
      "loss:    0.360984  [14000/20240]\n",
      "loss:    1.308240  [14100/20240]\n",
      "loss:    0.767574  [14200/20240]\n",
      "loss:    0.881928  [14300/20240]\n",
      "loss:    0.749663  [14400/20240]\n",
      "loss:    0.924617  [14500/20240]\n",
      "loss:    1.436539  [14600/20240]\n",
      "loss:    1.192303  [14700/20240]\n",
      "loss:    1.131073  [14800/20240]\n",
      "loss:    0.383955  [14900/20240]\n",
      "loss:    1.102946  [15000/20240]\n",
      "loss:    1.456881  [15100/20240]\n",
      "loss:    0.838454  [15200/20240]\n",
      "loss:    0.343425  [15300/20240]\n",
      "loss:    0.523611  [15400/20240]\n",
      "loss:    1.852281  [15500/20240]\n",
      "loss:    1.023938  [15600/20240]\n",
      "loss:    1.215342  [15700/20240]\n",
      "loss:    1.980483  [15800/20240]\n",
      "loss:    1.565422  [15900/20240]\n",
      "loss:    0.559487  [16000/20240]\n",
      "loss:    0.845755  [16100/20240]\n",
      "loss:    1.081992  [16200/20240]\n",
      "loss:    1.024625  [16300/20240]\n",
      "loss:    1.181681  [16400/20240]\n",
      "loss:    1.098630  [16500/20240]\n",
      "loss:    0.989712  [16600/20240]\n",
      "loss:    0.490136  [16700/20240]\n",
      "loss:    1.306287  [16800/20240]\n",
      "loss:    0.340829  [16900/20240]\n",
      "loss:    0.981359  [17000/20240]\n",
      "loss:    1.480248  [17100/20240]\n",
      "loss:    1.158979  [17200/20240]\n",
      "loss:    0.528805  [17300/20240]\n",
      "loss:    0.916241  [17400/20240]\n",
      "loss:    0.860501  [17500/20240]\n",
      "loss:    0.793927  [17600/20240]\n",
      "loss:    0.760414  [17700/20240]\n",
      "loss:    0.818639  [17800/20240]\n",
      "loss:    1.695144  [17900/20240]\n",
      "loss:    0.869569  [18000/20240]\n",
      "loss:    0.808694  [18100/20240]\n",
      "loss:    0.784186  [18200/20240]\n",
      "loss:    0.729673  [18300/20240]\n",
      "loss:    0.814774  [18400/20240]\n",
      "loss:    1.480789  [18500/20240]\n",
      "loss:    0.712867  [18600/20240]\n",
      "loss:    0.358387  [18700/20240]\n",
      "loss:    1.931282  [18800/20240]\n",
      "loss:    0.964051  [18900/20240]\n",
      "loss:    1.382046  [19000/20240]\n",
      "loss:    1.550525  [19100/20240]\n",
      "loss:    1.510202  [19200/20240]\n",
      "loss:    0.498680  [19300/20240]\n",
      "loss:    1.013467  [19400/20240]\n",
      "loss:    2.012093  [19500/20240]\n",
      "loss:    1.198776  [19600/20240]\n",
      "loss:    1.164507  [19700/20240]\n",
      "loss:    0.659038  [19800/20240]\n",
      "loss:    1.320679  [19900/20240]\n",
      "loss:    1.018113  [20000/20240]\n",
      "loss:    1.119733  [20100/20240]\n",
      "loss:    0.364956  [20200/20240]\n",
      "Test: \n",
      " Accuracy: 48.4%, Avg loss: 1.041004 \n",
      "\n",
      "======================================================\n",
      "epoch 6\n",
      "loss:    0.435990  [    0/20240]\n",
      "loss:    0.768333  [  100/20240]\n",
      "loss:    0.432293  [  200/20240]\n",
      "loss:    0.441136  [  300/20240]\n",
      "loss:    1.734335  [  400/20240]\n",
      "loss:    0.693630  [  500/20240]\n",
      "loss:    0.835263  [  600/20240]\n",
      "loss:    0.922483  [  700/20240]\n",
      "loss:    1.097476  [  800/20240]\n",
      "loss:    1.850363  [  900/20240]\n",
      "loss:    1.314404  [ 1000/20240]\n",
      "loss:    1.210608  [ 1100/20240]\n",
      "loss:    0.601731  [ 1200/20240]\n",
      "loss:    1.094849  [ 1300/20240]\n",
      "loss:    0.325182  [ 1400/20240]\n",
      "loss:    1.324817  [ 1500/20240]\n",
      "loss:    1.296149  [ 1600/20240]\n",
      "loss:    1.178241  [ 1700/20240]\n",
      "loss:    1.187506  [ 1800/20240]\n",
      "loss:    0.834356  [ 1900/20240]\n",
      "loss:    0.828461  [ 2000/20240]\n",
      "loss:    0.928557  [ 2100/20240]\n",
      "loss:    1.113575  [ 2200/20240]\n",
      "loss:    1.125199  [ 2300/20240]\n",
      "loss:    0.340411  [ 2400/20240]\n",
      "loss:    1.258452  [ 2500/20240]\n",
      "loss:    0.883590  [ 2600/20240]\n",
      "loss:    1.724374  [ 2700/20240]\n",
      "loss:    1.366669  [ 2800/20240]\n",
      "loss:    1.335282  [ 2900/20240]\n",
      "loss:    1.429130  [ 3000/20240]\n",
      "loss:    1.555313  [ 3100/20240]\n",
      "loss:    1.408043  [ 3200/20240]\n",
      "loss:    1.077096  [ 3300/20240]\n",
      "loss:    1.347379  [ 3400/20240]\n",
      "loss:    0.613547  [ 3500/20240]\n",
      "loss:    1.078995  [ 3600/20240]\n",
      "loss:    1.107224  [ 3700/20240]\n",
      "loss:    0.947158  [ 3800/20240]\n",
      "loss:    0.956349  [ 3900/20240]\n",
      "loss:    1.142402  [ 4000/20240]\n",
      "loss:    1.681619  [ 4100/20240]\n",
      "loss:    1.044039  [ 4200/20240]\n",
      "loss:    1.214875  [ 4300/20240]\n",
      "loss:    0.871429  [ 4400/20240]\n",
      "loss:    0.244484  [ 4500/20240]\n",
      "loss:    1.266635  [ 4600/20240]\n",
      "loss:    1.074931  [ 4700/20240]\n",
      "loss:    0.778095  [ 4800/20240]\n",
      "loss:    1.649469  [ 4900/20240]\n",
      "loss:    1.499591  [ 5000/20240]\n",
      "loss:    0.436582  [ 5100/20240]\n",
      "loss:    1.107567  [ 5200/20240]\n",
      "loss:    0.686161  [ 5300/20240]\n",
      "loss:    1.023688  [ 5400/20240]\n",
      "loss:    0.866535  [ 5500/20240]\n",
      "loss:    0.924521  [ 5600/20240]\n",
      "loss:    1.291436  [ 5700/20240]\n",
      "loss:    1.155533  [ 5800/20240]\n",
      "loss:    1.178225  [ 5900/20240]\n",
      "loss:    0.371725  [ 6000/20240]\n",
      "loss:    1.331194  [ 6100/20240]\n",
      "loss:    0.705872  [ 6200/20240]\n",
      "loss:    0.785285  [ 6300/20240]\n",
      "loss:    1.415006  [ 6400/20240]\n",
      "loss:    0.573669  [ 6500/20240]\n",
      "loss:    0.985654  [ 6600/20240]\n",
      "loss:    0.892329  [ 6700/20240]\n",
      "loss:    0.894878  [ 6800/20240]\n",
      "loss:    1.055805  [ 6900/20240]\n",
      "loss:    0.305953  [ 7000/20240]\n",
      "loss:    0.758796  [ 7100/20240]\n",
      "loss:    1.021976  [ 7200/20240]\n",
      "loss:    0.831623  [ 7300/20240]\n",
      "loss:    1.183226  [ 7400/20240]\n",
      "loss:    0.464017  [ 7500/20240]\n",
      "loss:    0.908597  [ 7600/20240]\n",
      "loss:    1.326896  [ 7700/20240]\n",
      "loss:    2.065520  [ 7800/20240]\n",
      "loss:    1.239049  [ 7900/20240]\n",
      "loss:    0.903694  [ 8000/20240]\n",
      "loss:    2.064935  [ 8100/20240]\n",
      "loss:    2.033749  [ 8200/20240]\n",
      "loss:    0.270833  [ 8300/20240]\n",
      "loss:    0.729881  [ 8400/20240]\n",
      "loss:    0.925040  [ 8500/20240]\n",
      "loss:    0.643446  [ 8600/20240]\n",
      "loss:    1.175838  [ 8700/20240]\n",
      "loss:    0.475016  [ 8800/20240]\n",
      "loss:    1.936691  [ 8900/20240]\n",
      "loss:    1.840353  [ 9000/20240]\n",
      "loss:    0.364219  [ 9100/20240]\n",
      "loss:    0.336317  [ 9200/20240]\n",
      "loss:    0.764436  [ 9300/20240]\n",
      "loss:    1.394689  [ 9400/20240]\n",
      "loss:    1.279154  [ 9500/20240]\n",
      "loss:    0.644064  [ 9600/20240]\n",
      "loss:    0.737957  [ 9700/20240]\n",
      "loss:    0.636909  [ 9800/20240]\n",
      "loss:    2.148508  [ 9900/20240]\n",
      "loss:    1.689763  [10000/20240]\n",
      "loss:    0.525613  [10100/20240]\n",
      "loss:    0.976081  [10200/20240]\n",
      "loss:    0.816234  [10300/20240]\n",
      "loss:    0.989528  [10400/20240]\n",
      "loss:    0.244398  [10500/20240]\n",
      "loss:    0.890170  [10600/20240]\n",
      "loss:    0.279278  [10700/20240]\n",
      "loss:    0.756043  [10800/20240]\n",
      "loss:    0.340874  [10900/20240]\n",
      "loss:    0.705843  [11000/20240]\n",
      "loss:    1.220034  [11100/20240]\n",
      "loss:    1.795490  [11200/20240]\n",
      "loss:    1.523346  [11300/20240]\n",
      "loss:    1.062044  [11400/20240]\n",
      "loss:    0.934004  [11500/20240]\n",
      "loss:    0.936331  [11600/20240]\n",
      "loss:    1.535565  [11700/20240]\n",
      "loss:    1.577117  [11800/20240]\n",
      "loss:    1.532601  [11900/20240]\n",
      "loss:    1.120461  [12000/20240]\n",
      "loss:    0.744661  [12100/20240]\n",
      "loss:    0.924044  [12200/20240]\n",
      "loss:    1.108975  [12300/20240]\n",
      "loss:    0.837141  [12400/20240]\n",
      "loss:    0.762999  [12500/20240]\n",
      "loss:    0.854704  [12600/20240]\n",
      "loss:    1.362511  [12700/20240]\n",
      "loss:    0.293318  [12800/20240]\n",
      "loss:    0.509514  [12900/20240]\n",
      "loss:    0.843552  [13000/20240]\n",
      "loss:    0.713249  [13100/20240]\n",
      "loss:    1.591346  [13200/20240]\n",
      "loss:    0.815738  [13300/20240]\n",
      "loss:    1.320200  [13400/20240]\n",
      "loss:    1.013400  [13500/20240]\n",
      "loss:    1.717738  [13600/20240]\n",
      "loss:    1.643826  [13700/20240]\n",
      "loss:    1.116593  [13800/20240]\n",
      "loss:    0.520258  [13900/20240]\n",
      "loss:    0.315891  [14000/20240]\n",
      "loss:    1.358406  [14100/20240]\n",
      "loss:    0.767149  [14200/20240]\n",
      "loss:    0.892786  [14300/20240]\n",
      "loss:    0.671404  [14400/20240]\n",
      "loss:    0.371946  [14500/20240]\n",
      "loss:    0.745162  [14600/20240]\n",
      "loss:    1.526386  [14700/20240]\n",
      "loss:    0.752728  [14800/20240]\n",
      "loss:    1.137018  [14900/20240]\n",
      "loss:    0.699578  [15000/20240]\n",
      "loss:    1.100927  [15100/20240]\n",
      "loss:    1.343707  [15200/20240]\n",
      "loss:    0.912694  [15300/20240]\n",
      "loss:    0.937907  [15400/20240]\n",
      "loss:    0.604185  [15500/20240]\n",
      "loss:    0.846609  [15600/20240]\n",
      "loss:    0.715949  [15700/20240]\n",
      "loss:    0.622440  [15800/20240]\n",
      "loss:    1.209182  [15900/20240]\n",
      "loss:    1.287711  [16000/20240]\n",
      "loss:    0.712672  [16100/20240]\n",
      "loss:    1.428359  [16200/20240]\n",
      "loss:    1.662176  [16300/20240]\n",
      "loss:    1.806670  [16400/20240]\n",
      "loss:    0.552148  [16500/20240]\n",
      "loss:    0.659994  [16600/20240]\n",
      "loss:    0.936486  [16700/20240]\n",
      "loss:    1.158933  [16800/20240]\n",
      "loss:    1.026612  [16900/20240]\n",
      "loss:    1.092471  [17000/20240]\n",
      "loss:    0.617905  [17100/20240]\n",
      "loss:    0.283987  [17200/20240]\n",
      "loss:    0.432114  [17300/20240]\n",
      "loss:    1.399437  [17400/20240]\n",
      "loss:    1.032968  [17500/20240]\n",
      "loss:    1.078405  [17600/20240]\n",
      "loss:    1.295295  [17700/20240]\n",
      "loss:    0.724612  [17800/20240]\n",
      "loss:    0.723341  [17900/20240]\n",
      "loss:    1.000158  [18000/20240]\n",
      "loss:    0.899674  [18100/20240]\n",
      "loss:    1.546375  [18200/20240]\n",
      "loss:    0.741184  [18300/20240]\n",
      "loss:    0.475215  [18400/20240]\n",
      "loss:    1.249697  [18500/20240]\n",
      "loss:    0.295463  [18600/20240]\n",
      "loss:    1.440194  [18700/20240]\n",
      "loss:    1.393868  [18800/20240]\n",
      "loss:    0.546293  [18900/20240]\n",
      "loss:    0.413591  [19000/20240]\n",
      "loss:    1.240482  [19100/20240]\n",
      "loss:    1.579364  [19200/20240]\n",
      "loss:    0.891383  [19300/20240]\n",
      "loss:    0.634510  [19400/20240]\n",
      "loss:    0.928751  [19500/20240]\n",
      "loss:    0.435258  [19600/20240]\n",
      "loss:    0.732649  [19700/20240]\n",
      "loss:    1.086115  [19800/20240]\n",
      "loss:    0.904145  [19900/20240]\n",
      "loss:    0.758135  [20000/20240]\n",
      "loss:    0.520632  [20100/20240]\n",
      "loss:    0.880426  [20200/20240]\n",
      "Test: \n",
      " Accuracy: 45.5%, Avg loss: 1.060317 \n",
      "\n",
      "======================================================\n",
      "epoch 7\n",
      "loss:    1.568717  [    0/20240]\n",
      "loss:    0.602963  [  100/20240]\n",
      "loss:    1.310957  [  200/20240]\n",
      "loss:    1.335805  [  300/20240]\n",
      "loss:    1.247088  [  400/20240]\n",
      "loss:    0.452917  [  500/20240]\n",
      "loss:    0.776485  [  600/20240]\n",
      "loss:    1.120866  [  700/20240]\n",
      "loss:    0.313899  [  800/20240]\n",
      "loss:    0.992748  [  900/20240]\n",
      "loss:    1.095963  [ 1000/20240]\n",
      "loss:    1.072482  [ 1100/20240]\n",
      "loss:    1.657728  [ 1200/20240]\n",
      "loss:    0.469532  [ 1300/20240]\n",
      "loss:    1.438648  [ 1400/20240]\n",
      "loss:    0.926851  [ 1500/20240]\n",
      "loss:    1.097591  [ 1600/20240]\n",
      "loss:    1.482316  [ 1700/20240]\n",
      "loss:    0.778151  [ 1800/20240]\n",
      "loss:    0.878521  [ 1900/20240]\n",
      "loss:    0.940272  [ 2000/20240]\n",
      "loss:    0.859553  [ 2100/20240]\n",
      "loss:    1.000574  [ 2200/20240]\n",
      "loss:    1.164301  [ 2300/20240]\n",
      "loss:    0.791785  [ 2400/20240]\n",
      "loss:    0.268931  [ 2500/20240]\n",
      "loss:    0.382038  [ 2600/20240]\n",
      "loss:    0.634991  [ 2700/20240]\n",
      "loss:    1.166471  [ 2800/20240]\n",
      "loss:    0.829275  [ 2900/20240]\n",
      "loss:    1.044877  [ 3000/20240]\n",
      "loss:    0.780465  [ 3100/20240]\n",
      "loss:    1.891039  [ 3200/20240]\n",
      "loss:    0.625763  [ 3300/20240]\n",
      "loss:    0.462518  [ 3400/20240]\n",
      "loss:    0.890619  [ 3500/20240]\n",
      "loss:    1.279221  [ 3600/20240]\n",
      "loss:    0.624757  [ 3700/20240]\n",
      "loss:    0.590823  [ 3800/20240]\n",
      "loss:    1.895923  [ 3900/20240]\n",
      "loss:    0.711525  [ 4000/20240]\n",
      "loss:    1.431738  [ 4100/20240]\n",
      "loss:    1.068745  [ 4200/20240]\n",
      "loss:    1.220819  [ 4300/20240]\n",
      "loss:    1.005526  [ 4400/20240]\n",
      "loss:    0.621641  [ 4500/20240]\n",
      "loss:    1.142175  [ 4600/20240]\n",
      "loss:    0.508861  [ 4700/20240]\n",
      "loss:    0.904137  [ 4800/20240]\n",
      "loss:    0.599925  [ 4900/20240]\n",
      "loss:    1.335909  [ 5000/20240]\n",
      "loss:    1.489540  [ 5100/20240]\n",
      "loss:    0.951520  [ 5200/20240]\n",
      "loss:    0.908245  [ 5300/20240]\n",
      "loss:    0.545203  [ 5400/20240]\n",
      "loss:    1.511927  [ 5500/20240]\n",
      "loss:    0.619622  [ 5600/20240]\n",
      "loss:    1.040014  [ 5700/20240]\n",
      "loss:    0.669298  [ 5800/20240]\n",
      "loss:    0.994084  [ 5900/20240]\n",
      "loss:    0.783103  [ 6000/20240]\n",
      "loss:    0.283543  [ 6100/20240]\n",
      "loss:    0.569728  [ 6200/20240]\n",
      "loss:    0.697057  [ 6300/20240]\n",
      "loss:    0.668659  [ 6400/20240]\n",
      "loss:    0.294185  [ 6500/20240]\n",
      "loss:    0.750162  [ 6600/20240]\n",
      "loss:    0.821093  [ 6700/20240]\n",
      "loss:    0.629479  [ 6800/20240]\n",
      "loss:    0.860137  [ 6900/20240]\n",
      "loss:    1.174582  [ 7000/20240]\n",
      "loss:    1.012267  [ 7100/20240]\n",
      "loss:    1.800474  [ 7200/20240]\n",
      "loss:    0.746421  [ 7300/20240]\n",
      "loss:    0.784439  [ 7400/20240]\n",
      "loss:    1.219236  [ 7500/20240]\n",
      "loss:    1.689147  [ 7600/20240]\n",
      "loss:    1.055444  [ 7700/20240]\n",
      "loss:    1.148280  [ 7800/20240]\n",
      "loss:    1.386957  [ 7900/20240]\n",
      "loss:    0.540687  [ 8000/20240]\n",
      "loss:    0.415507  [ 8100/20240]\n",
      "loss:    1.102812  [ 8200/20240]\n",
      "loss:    0.499195  [ 8300/20240]\n",
      "loss:    0.778581  [ 8400/20240]\n",
      "loss:    0.913667  [ 8500/20240]\n",
      "loss:    0.768557  [ 8600/20240]\n",
      "loss:    1.120313  [ 8700/20240]\n",
      "loss:    0.846592  [ 8800/20240]\n",
      "loss:    1.102375  [ 8900/20240]\n",
      "loss:    0.566654  [ 9000/20240]\n",
      "loss:    1.446918  [ 9100/20240]\n",
      "loss:    0.832288  [ 9200/20240]\n",
      "loss:    0.947129  [ 9300/20240]\n",
      "loss:    0.844521  [ 9400/20240]\n",
      "loss:    0.425217  [ 9500/20240]\n",
      "loss:    0.516800  [ 9600/20240]\n",
      "loss:    0.863822  [ 9700/20240]\n",
      "loss:    0.264763  [ 9800/20240]\n",
      "loss:    0.306639  [ 9900/20240]\n",
      "loss:    0.473463  [10000/20240]\n",
      "loss:    1.928524  [10100/20240]\n",
      "loss:    0.862103  [10200/20240]\n",
      "loss:    0.923454  [10300/20240]\n",
      "loss:    0.783982  [10400/20240]\n",
      "loss:    0.946902  [10500/20240]\n",
      "loss:    0.371196  [10600/20240]\n",
      "loss:    0.565446  [10700/20240]\n",
      "loss:    0.719719  [10800/20240]\n",
      "loss:    1.475857  [10900/20240]\n",
      "loss:    1.032133  [11000/20240]\n",
      "loss:    0.925994  [11100/20240]\n",
      "loss:    0.715206  [11200/20240]\n",
      "loss:    1.324695  [11300/20240]\n",
      "loss:    1.588928  [11400/20240]\n",
      "loss:    0.959724  [11500/20240]\n",
      "loss:    1.898005  [11600/20240]\n",
      "loss:    1.752846  [11700/20240]\n",
      "loss:    0.235024  [11800/20240]\n",
      "loss:    0.581419  [11900/20240]\n",
      "loss:    1.041245  [12000/20240]\n",
      "loss:    0.736159  [12100/20240]\n",
      "loss:    1.153052  [12200/20240]\n",
      "loss:    1.290540  [12300/20240]\n",
      "loss:    0.751516  [12400/20240]\n",
      "loss:    1.204295  [12500/20240]\n",
      "loss:    0.849334  [12600/20240]\n",
      "loss:    1.109603  [12700/20240]\n",
      "loss:    0.494301  [12800/20240]\n",
      "loss:    1.816485  [12900/20240]\n",
      "loss:    0.898371  [13000/20240]\n",
      "loss:    1.010513  [13100/20240]\n",
      "loss:    0.715627  [13200/20240]\n",
      "loss:    1.708404  [13300/20240]\n",
      "loss:    1.941121  [13400/20240]\n",
      "loss:    1.603002  [13500/20240]\n",
      "loss:    0.916736  [13600/20240]\n",
      "loss:    0.456575  [13700/20240]\n",
      "loss:    0.523048  [13800/20240]\n",
      "loss:    1.937745  [13900/20240]\n",
      "loss:    1.217893  [14000/20240]\n",
      "loss:    0.439284  [14100/20240]\n",
      "loss:    0.506143  [14200/20240]\n",
      "loss:    0.488684  [14300/20240]\n",
      "loss:    0.901616  [14400/20240]\n",
      "loss:    0.519001  [14500/20240]\n",
      "loss:    0.854512  [14600/20240]\n",
      "loss:    1.239344  [14700/20240]\n",
      "loss:    1.232861  [14800/20240]\n",
      "loss:    0.927788  [14900/20240]\n",
      "loss:    1.223459  [15000/20240]\n",
      "loss:    1.070597  [15100/20240]\n",
      "loss:    0.833488  [15200/20240]\n",
      "loss:    1.022323  [15300/20240]\n",
      "loss:    0.774570  [15400/20240]\n",
      "loss:    2.170220  [15500/20240]\n",
      "loss:    0.724798  [15600/20240]\n",
      "loss:    0.900171  [15700/20240]\n",
      "loss:    0.783228  [15800/20240]\n",
      "loss:    0.607605  [15900/20240]\n",
      "loss:    0.812970  [16000/20240]\n",
      "loss:    0.294638  [16100/20240]\n",
      "loss:    0.524383  [16200/20240]\n",
      "loss:    1.054028  [16300/20240]\n",
      "loss:    0.712748  [16400/20240]\n",
      "loss:    0.983910  [16500/20240]\n",
      "loss:    1.233789  [16600/20240]\n",
      "loss:    1.478606  [16700/20240]\n",
      "loss:    0.273682  [16800/20240]\n",
      "loss:    0.852533  [16900/20240]\n",
      "loss:    0.267159  [17000/20240]\n",
      "loss:    0.435023  [17100/20240]\n",
      "loss:    1.103444  [17200/20240]\n",
      "loss:    0.924083  [17300/20240]\n",
      "loss:    0.981805  [17400/20240]\n",
      "loss:    0.744796  [17500/20240]\n",
      "loss:    1.005089  [17600/20240]\n",
      "loss:    1.113600  [17700/20240]\n",
      "loss:    0.618569  [17800/20240]\n",
      "loss:    1.770192  [17900/20240]\n",
      "loss:    0.793589  [18000/20240]\n",
      "loss:    1.210376  [18100/20240]\n",
      "loss:    1.048694  [18200/20240]\n",
      "loss:    1.183452  [18300/20240]\n",
      "loss:    1.648167  [18400/20240]\n",
      "loss:    0.232654  [18500/20240]\n",
      "loss:    0.986280  [18600/20240]\n",
      "loss:    1.655778  [18700/20240]\n",
      "loss:    0.716961  [18800/20240]\n",
      "loss:    2.281916  [18900/20240]\n",
      "loss:    0.413270  [19000/20240]\n",
      "loss:    0.838157  [19100/20240]\n",
      "loss:    1.084252  [19200/20240]\n",
      "loss:    0.743921  [19300/20240]\n",
      "loss:    0.311881  [19400/20240]\n",
      "loss:    1.292374  [19500/20240]\n",
      "loss:    0.315971  [19600/20240]\n",
      "loss:    1.622192  [19700/20240]\n",
      "loss:    0.289373  [19800/20240]\n",
      "loss:    0.839036  [19900/20240]\n",
      "loss:    1.082336  [20000/20240]\n",
      "loss:    0.487320  [20100/20240]\n",
      "loss:    0.275362  [20200/20240]\n",
      "Test: \n",
      " Accuracy: 45.7%, Avg loss: 1.091573 \n",
      "\n",
      "======================================================\n",
      "epoch 8\n",
      "loss:    1.394497  [    0/20240]\n",
      "loss:    0.225825  [  100/20240]\n",
      "loss:    1.233019  [  200/20240]\n",
      "loss:    1.178506  [  300/20240]\n",
      "loss:    0.936398  [  400/20240]\n",
      "loss:    0.357265  [  500/20240]\n",
      "loss:    0.716271  [  600/20240]\n",
      "loss:    1.041888  [  700/20240]\n",
      "loss:    1.807492  [  800/20240]\n",
      "loss:    0.740180  [  900/20240]\n",
      "loss:    0.603264  [ 1000/20240]\n",
      "loss:    0.804864  [ 1100/20240]\n",
      "loss:    1.238578  [ 1200/20240]\n",
      "loss:    0.883136  [ 1300/20240]\n",
      "loss:    0.908578  [ 1400/20240]\n",
      "loss:    0.428797  [ 1500/20240]\n",
      "loss:    1.354783  [ 1600/20240]\n",
      "loss:    0.557634  [ 1700/20240]\n",
      "loss:    1.496754  [ 1800/20240]\n",
      "loss:    1.251412  [ 1900/20240]\n",
      "loss:    0.790822  [ 2000/20240]\n",
      "loss:    1.314042  [ 2100/20240]\n",
      "loss:    1.502859  [ 2200/20240]\n",
      "loss:    1.836847  [ 2300/20240]\n",
      "loss:    1.332937  [ 2400/20240]\n",
      "loss:    0.806108  [ 2500/20240]\n",
      "loss:    0.725087  [ 2600/20240]\n",
      "loss:    0.517456  [ 2700/20240]\n",
      "loss:    1.366261  [ 2800/20240]\n",
      "loss:    0.231274  [ 2900/20240]\n",
      "loss:    0.491297  [ 3000/20240]\n",
      "loss:    1.918392  [ 3100/20240]\n",
      "loss:    0.708624  [ 3200/20240]\n",
      "loss:    0.461979  [ 3300/20240]\n",
      "loss:    2.198038  [ 3400/20240]\n",
      "loss:    1.276963  [ 3500/20240]\n",
      "loss:    1.984833  [ 3600/20240]\n",
      "loss:    0.609906  [ 3700/20240]\n",
      "loss:    1.374629  [ 3800/20240]\n",
      "loss:    0.634875  [ 3900/20240]\n",
      "loss:    1.707256  [ 4000/20240]\n",
      "loss:    0.366042  [ 4100/20240]\n",
      "loss:    0.820037  [ 4200/20240]\n",
      "loss:    0.330517  [ 4300/20240]\n",
      "loss:    1.133081  [ 4400/20240]\n",
      "loss:    0.728874  [ 4500/20240]\n",
      "loss:    0.463737  [ 4600/20240]\n",
      "loss:    1.744829  [ 4700/20240]\n",
      "loss:    0.574204  [ 4800/20240]\n",
      "loss:    1.248697  [ 4900/20240]\n",
      "loss:    1.536495  [ 5000/20240]\n",
      "loss:    2.110751  [ 5100/20240]\n",
      "loss:    0.575329  [ 5200/20240]\n",
      "loss:    0.847885  [ 5300/20240]\n",
      "loss:    0.641732  [ 5400/20240]\n",
      "loss:    0.488744  [ 5500/20240]\n",
      "loss:    0.737191  [ 5600/20240]\n",
      "loss:    0.366191  [ 5700/20240]\n",
      "loss:    1.092111  [ 5800/20240]\n",
      "loss:    0.713114  [ 5900/20240]\n",
      "loss:    0.875566  [ 6000/20240]\n",
      "loss:    0.763107  [ 6100/20240]\n",
      "loss:    0.976888  [ 6200/20240]\n",
      "loss:    0.433025  [ 6300/20240]\n",
      "loss:    0.940394  [ 6400/20240]\n",
      "loss:    0.894206  [ 6500/20240]\n",
      "loss:    0.927954  [ 6600/20240]\n",
      "loss:    0.840168  [ 6700/20240]\n",
      "loss:    0.833768  [ 6800/20240]\n",
      "loss:    0.331568  [ 6900/20240]\n",
      "loss:    1.165837  [ 7000/20240]\n",
      "loss:    1.414737  [ 7100/20240]\n",
      "loss:    1.246063  [ 7200/20240]\n",
      "loss:    0.748462  [ 7300/20240]\n",
      "loss:    1.152946  [ 7400/20240]\n",
      "loss:    0.967824  [ 7500/20240]\n",
      "loss:    1.249882  [ 7600/20240]\n",
      "loss:    0.685637  [ 7700/20240]\n",
      "loss:    1.013226  [ 7800/20240]\n",
      "loss:    1.165134  [ 7900/20240]\n",
      "loss:    0.674534  [ 8000/20240]\n",
      "loss:    0.391472  [ 8100/20240]\n",
      "loss:    0.947981  [ 8200/20240]\n",
      "loss:    0.760295  [ 8300/20240]\n",
      "loss:    1.452341  [ 8400/20240]\n",
      "loss:    1.443349  [ 8500/20240]\n",
      "loss:    1.058698  [ 8600/20240]\n",
      "loss:    1.403533  [ 8700/20240]\n",
      "loss:    1.071235  [ 8800/20240]\n",
      "loss:    0.785221  [ 8900/20240]\n",
      "loss:    1.890043  [ 9000/20240]\n",
      "loss:    1.320584  [ 9100/20240]\n",
      "loss:    0.878754  [ 9200/20240]\n",
      "loss:    1.212347  [ 9300/20240]\n",
      "loss:    0.809241  [ 9400/20240]\n",
      "loss:    0.651728  [ 9500/20240]\n",
      "loss:    1.079257  [ 9600/20240]\n",
      "loss:    1.009942  [ 9700/20240]\n",
      "loss:    1.563731  [ 9800/20240]\n",
      "loss:    1.535115  [ 9900/20240]\n",
      "loss:    0.899725  [10000/20240]\n",
      "loss:    1.072641  [10100/20240]\n",
      "loss:    0.715435  [10200/20240]\n",
      "loss:    1.524874  [10300/20240]\n",
      "loss:    0.490122  [10400/20240]\n",
      "loss:    0.678372  [10500/20240]\n",
      "loss:    1.015753  [10600/20240]\n",
      "loss:    1.780251  [10700/20240]\n",
      "loss:    1.206329  [10800/20240]\n",
      "loss:    0.411887  [10900/20240]\n",
      "loss:    0.530620  [11000/20240]\n",
      "loss:    0.674831  [11100/20240]\n",
      "loss:    1.076525  [11200/20240]\n",
      "loss:    0.481781  [11300/20240]\n",
      "loss:    1.929367  [11400/20240]\n",
      "loss:    2.167437  [11500/20240]\n",
      "loss:    0.850002  [11600/20240]\n",
      "loss:    0.520208  [11700/20240]\n",
      "loss:    1.415963  [11800/20240]\n",
      "loss:    0.364567  [11900/20240]\n",
      "loss:    0.910374  [12000/20240]\n",
      "loss:    0.925036  [12100/20240]\n",
      "loss:    1.096381  [12200/20240]\n",
      "loss:    0.924376  [12300/20240]\n",
      "loss:    1.086993  [12400/20240]\n",
      "loss:    0.264052  [12500/20240]\n",
      "loss:    0.995029  [12600/20240]\n",
      "loss:    0.571190  [12700/20240]\n",
      "loss:    0.914970  [12800/20240]\n",
      "loss:    1.325119  [12900/20240]\n",
      "loss:    0.209791  [13000/20240]\n",
      "loss:    1.302582  [13100/20240]\n",
      "loss:    0.943743  [13200/20240]\n",
      "loss:    0.314152  [13300/20240]\n",
      "loss:    1.823377  [13400/20240]\n",
      "loss:    0.418264  [13500/20240]\n",
      "loss:    2.035154  [13600/20240]\n",
      "loss:    1.561607  [13700/20240]\n",
      "loss:    0.257118  [13800/20240]\n",
      "loss:    1.907764  [13900/20240]\n",
      "loss:    1.284099  [14000/20240]\n",
      "loss:    1.090491  [14100/20240]\n",
      "loss:    0.735164  [14200/20240]\n",
      "loss:    0.433365  [14300/20240]\n",
      "loss:    1.189866  [14400/20240]\n",
      "loss:    0.360662  [14500/20240]\n",
      "loss:    0.671224  [14600/20240]\n",
      "loss:    1.732385  [14700/20240]\n",
      "loss:    0.213547  [14800/20240]\n",
      "loss:    1.182928  [14900/20240]\n",
      "loss:    1.413539  [15000/20240]\n",
      "loss:    0.407835  [15100/20240]\n",
      "loss:    1.019598  [15200/20240]\n",
      "loss:    1.037927  [15300/20240]\n",
      "loss:    0.580315  [15400/20240]\n",
      "loss:    0.816566  [15500/20240]\n",
      "loss:    0.277561  [15600/20240]\n",
      "loss:    2.308595  [15700/20240]\n",
      "loss:    0.750356  [15800/20240]\n",
      "loss:    0.635916  [15900/20240]\n",
      "loss:    1.227918  [16000/20240]\n",
      "loss:    0.592661  [16100/20240]\n",
      "loss:    0.332711  [16200/20240]\n",
      "loss:    0.835197  [16300/20240]\n",
      "loss:    0.826350  [16400/20240]\n",
      "loss:    0.463846  [16500/20240]\n",
      "loss:    1.100095  [16600/20240]\n",
      "loss:    0.563186  [16700/20240]\n",
      "loss:    0.755934  [16800/20240]\n",
      "loss:    0.552964  [16900/20240]\n",
      "loss:    0.812948  [17000/20240]\n",
      "loss:    1.453305  [17100/20240]\n",
      "loss:    2.217992  [17200/20240]\n",
      "loss:    0.989888  [17300/20240]\n",
      "loss:    1.776514  [17400/20240]\n",
      "loss:    0.628635  [17500/20240]\n",
      "loss:    0.434137  [17600/20240]\n",
      "loss:    0.677772  [17700/20240]\n",
      "loss:    1.087910  [17800/20240]\n",
      "loss:    1.013831  [17900/20240]\n",
      "loss:    1.236377  [18000/20240]\n",
      "loss:    0.480007  [18100/20240]\n",
      "loss:    0.541724  [18200/20240]\n",
      "loss:    1.182578  [18300/20240]\n",
      "loss:    1.485264  [18400/20240]\n",
      "loss:    1.614210  [18500/20240]\n",
      "loss:    1.696200  [18600/20240]\n",
      "loss:    2.269494  [18700/20240]\n",
      "loss:    0.738541  [18800/20240]\n",
      "loss:    0.759367  [18900/20240]\n",
      "loss:    0.366971  [19000/20240]\n",
      "loss:    1.070265  [19100/20240]\n",
      "loss:    0.971184  [19200/20240]\n",
      "loss:    0.598779  [19300/20240]\n",
      "loss:    0.723515  [19400/20240]\n",
      "loss:    0.254124  [19500/20240]\n",
      "loss:    1.916096  [19600/20240]\n",
      "loss:    1.243097  [19700/20240]\n",
      "loss:    0.619784  [19800/20240]\n",
      "loss:    0.249570  [19900/20240]\n",
      "loss:    1.203964  [20000/20240]\n",
      "loss:    0.355997  [20100/20240]\n",
      "loss:    0.768890  [20200/20240]\n",
      "Test: \n",
      " Accuracy: 48.7%, Avg loss: 1.046408 \n",
      "\n",
      "======================================================\n",
      "epoch 9\n",
      "loss:    1.588562  [    0/20240]\n",
      "loss:    0.277672  [  100/20240]\n",
      "loss:    0.823870  [  200/20240]\n",
      "loss:    0.414451  [  300/20240]\n",
      "loss:    1.019673  [  400/20240]\n",
      "loss:    0.948573  [  500/20240]\n",
      "loss:    0.952525  [  600/20240]\n",
      "loss:    0.246714  [  700/20240]\n",
      "loss:    0.999349  [  800/20240]\n",
      "loss:    0.229568  [  900/20240]\n",
      "loss:    1.042422  [ 1000/20240]\n",
      "loss:    0.937759  [ 1100/20240]\n",
      "loss:    0.496786  [ 1200/20240]\n",
      "loss:    1.314327  [ 1300/20240]\n",
      "loss:    1.031168  [ 1400/20240]\n",
      "loss:    0.633682  [ 1500/20240]\n",
      "loss:    0.671262  [ 1600/20240]\n",
      "loss:    0.853780  [ 1700/20240]\n",
      "loss:    0.269861  [ 1800/20240]\n",
      "loss:    0.965050  [ 1900/20240]\n",
      "loss:    1.622088  [ 2000/20240]\n",
      "loss:    0.674196  [ 2100/20240]\n",
      "loss:    0.741331  [ 2200/20240]\n",
      "loss:    1.084476  [ 2300/20240]\n",
      "loss:    0.808934  [ 2400/20240]\n",
      "loss:    0.294246  [ 2500/20240]\n",
      "loss:    1.382308  [ 2600/20240]\n",
      "loss:    1.254741  [ 2700/20240]\n",
      "loss:    1.350986  [ 2800/20240]\n",
      "loss:    1.857333  [ 2900/20240]\n",
      "loss:    0.811362  [ 3000/20240]\n",
      "loss:    0.827709  [ 3100/20240]\n",
      "loss:    0.730940  [ 3200/20240]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15304\\3525906956.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_train_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_glove\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mtest_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_test_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_glove\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15304\\1009997982.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# Backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m#correct += (pred.argmax(1) == y.reshape(1, batch_size).to(device)).type(torch.float).sum().item()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[1;32m--> 488\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print('======================================================')\n",
    "    print('epoch '+str(i))\n",
    "    \n",
    "    train_loop(glove_train_loader, network_glove, criterion, optimizer)\n",
    "    test_loop(glove_test_loader, network_glove, criterion)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e18124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(network_glove,\"GloveModel.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f0df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d89a3db",
   "metadata": {},
   "source": [
    "# Bert and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "547b3126",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxl=18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1ae8f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "english_train_add = \"./MSCTD_dataset/english_train.txt\"\n",
    "english_test_add = \"./MSCTD_dataset/english_test.txt\"\n",
    "\n",
    "sentiment_train_add = \"./MSCTD_dataset/sentiment_train.txt\"\n",
    "sentiment_test_add = \"./MSCTD_dataset/sentiment_test.txt\"\n",
    "\n",
    "corpus_text_train = []\n",
    "corpus_text_test = []\n",
    "\n",
    "corpus_sentiment_train = []\n",
    "corpus_sentiment_test = []\n",
    "\n",
    "#####################################################################################\n",
    "txt_file = open(english_train_add, encoding=\"utf8\")\n",
    "\n",
    "for line in txt_file:\n",
    "    a = line.strip()\n",
    "    corpus_text_train.append(re.sub(r'[^\\w\\s]','', a).lower())\n",
    "\n",
    "txt_file.close()\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "txt_file = open(english_test_add, encoding=\"utf8\")\n",
    "\n",
    "for line in txt_file:\n",
    "    a = line.strip()\n",
    "    corpus_text_test.append(re.sub(r'[^\\w\\s]','', a).lower())\n",
    "\n",
    "txt_file.close()\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "txt_file = open(sentiment_train_add, encoding=\"utf8\")\n",
    "\n",
    "for line in txt_file:\n",
    "    corpus_sentiment_train.append(line.strip())\n",
    "\n",
    "txt_file.close()\n",
    "    \n",
    "#####################################################################################\n",
    "txt_file = open(sentiment_test_add, encoding=\"utf8\")\n",
    "\n",
    "for line in txt_file:\n",
    "    corpus_sentiment_test.append(line.strip())\n",
    "    \n",
    "txt_file.close()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "98f9d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "665ab144",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2862d514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "class BertDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.labels = [dataset[i][1] for i in range(len(dataset))]\n",
    "        self.texts = [ tokenizer(dataset[i][0], padding='max_length', max_length = maxl, truncation=True, return_tensors=\"pt\")['input_ids'] for i in range(len(dataset))]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.texts[idx]\n",
    "        batch_y = torch.tensor(int(self.labels[idx]))\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "733011e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.fc1 = nn.Linear(768, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        out = self.bert(inputs)\n",
    "        output = self.fc1(out[1])\n",
    "        output = self.relu(output)\n",
    "        output = self.fc2(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.fc3(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bfb5eedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=768, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=3, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertclassifier = BertClassifier().to(device)\n",
    "bertclassifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ed371bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = [[corpus_text_train[x], corpus_sentiment_train[x]] for x in range(len(corpus_sentiment_train))]\n",
    "testset = [[corpus_text_test[x], corpus_sentiment_test[x]] for x in range(len(corpus_sentiment_test))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7d47c5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(bertclassifier.parameters(), lr=learning_rate, eps=1e-8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "901a20bc",
   "metadata": {},
   "outputs": [],
   "source": [
    " train, test = BertDataset(trainset), BertDataset(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bf0bc996",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_trainloader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True)\n",
    "bert_testloader = torch.utils.data.DataLoader(test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "03a9779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct=0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X=X.squeeze(1).to(device)\n",
    "        y=y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X=X.squeeze(1).to(device)\n",
    "            y=y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b1eb9686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.094602  [    0/20240]\n",
      "loss: 1.064679  [ 6400/20240]\n",
      "loss: 0.941850  [12800/20240]\n",
      "loss: 0.919585  [19200/20240]\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.932393 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.861210  [    0/20240]\n",
      "loss: 0.847534  [ 6400/20240]\n",
      "loss: 0.898930  [12800/20240]\n",
      "loss: 0.802000  [19200/20240]\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Avg loss: 0.877759 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.660818  [    0/20240]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15304\\1613112748.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert_trainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbertclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtest_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert_testloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbertclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15304\\779637540.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcorrect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(bert_trainloader, bertclassifier, loss_fn, optimizer)\n",
    "    test_loop(bert_testloader, bertclassifier, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cfea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bertclassifier,\"bertclassifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb4c3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0f2e285",
   "metadata": {},
   "source": [
    "# Evaluation and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8c7aed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfmodel = torch.load(\"tfidfModel.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e0dd19a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "svmmodel = torch.load(\"SVMmodel.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5d0839c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "glovemodel = torch.load(\"GloveModel.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1f322511",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertmodel = torch.load(\"bertclassifier.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb1dcdc",
   "metadata": {},
   "source": [
    "### accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3a3d40",
   "metadata": {},
   "source": [
    "from the previous parts, the accuracies are as follows :\n",
    "\n",
    "part 1, model based on tf-idf  : 49%\n",
    "\n",
    "part 2, model based on svm : 48%\n",
    "\n",
    "part 3, model based on GloVe : 48.7%\n",
    "\n",
    "part 4, model based on Bert : 61.4%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac08ac2e",
   "metadata": {},
   "source": [
    "### f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "65ab9634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(dataset, model):\n",
    "    predicted=[]\n",
    "    real=[]\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            X=dataset[i][0]\n",
    "            y=dataset[i][1].item()\n",
    "            pred = model(X.float()).argmax()\n",
    "            predicted.append(pred)\n",
    "            real.append(y)\n",
    "    return predicted, real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "82fb401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptfidf, rtfidf = evaluation(test_dataset1, tfidfmodel.to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "feefbfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "psvm, rsvm = evaluation(test_dataset2, svmmodel.to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a42708ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glovevaluation(dataset, model):\n",
    "    predicted=[]\n",
    "    real=[]\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            X=dataset[i][0].reshape(1,24,50).to(device)\n",
    "            y=dataset[i][1].item()\n",
    "            pred = model(X.float()).argmax()\n",
    "            predicted.append(pred)\n",
    "            real.append(y)\n",
    "    return predicted, real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "33cb05e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pglove, rglove = glovevaluation(glove_test_dataset, glovemodel.to(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "345e849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bertevaluation(dataset, model):\n",
    "    predicted=[]\n",
    "    real=[]\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            X=dataset[i][0]\n",
    "            y=dataset[i][1].item()\n",
    "            pred = model(X).argmax(1)[0].item()\n",
    "            predicted.append(pred)\n",
    "            real.append(y)\n",
    "    return predicted, real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "91feffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbert, rbert = bertevaluation(test, bertmodel.to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5af4d3f",
   "metadata": {},
   "source": [
    "#### part 1, model based on tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a97d54a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.48      0.39      1298\n",
      "           1       0.58      0.61      0.60      2163\n",
      "           2       0.59      0.34      0.43      1606\n",
      "\n",
      "    accuracy                           0.49      5067\n",
      "   macro avg       0.50      0.48      0.47      5067\n",
      "weighted avg       0.52      0.49      0.49      5067\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(classification_report(rtfidf, ptfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ff3a4",
   "metadata": {},
   "source": [
    "#### part 2, model based on svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "aeeff08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.52      0.42      1224\n",
      "           1       0.63      0.40      0.49      2076\n",
      "           2       0.47      0.53      0.50      1569\n",
      "\n",
      "    accuracy                           0.47      4869\n",
      "   macro avg       0.49      0.48      0.47      4869\n",
      "weighted avg       0.51      0.47      0.48      4869\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(rsvm, psvm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f605520d",
   "metadata": {},
   "source": [
    "#### part 3, model based on GloVe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "36f660f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.65      0.46      1298\n",
      "           1       0.57      0.55      0.56      2163\n",
      "           2       0.61      0.24      0.35      1606\n",
      "\n",
      "    accuracy                           0.48      5067\n",
      "   macro avg       0.51      0.48      0.46      5067\n",
      "weighted avg       0.53      0.48      0.47      5067\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(torch.tensor(rglove).to(\"cpu\"), torch.tensor(pglove).to(\"cpu\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2a2fc7",
   "metadata": {},
   "source": [
    "#### part 4, model based on Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3f26a008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.62      0.52      1298\n",
      "           1       0.70      0.66      0.68      2163\n",
      "           2       0.66      0.50      0.57      1606\n",
      "\n",
      "    accuracy                           0.60      5067\n",
      "   macro avg       0.60      0.59      0.59      5067\n",
      "weighted avg       0.62      0.60      0.60      5067\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(rbert, pbert))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea1ff95",
   "metadata": {},
   "source": [
    "### confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd58bb5",
   "metadata": {},
   "source": [
    "#### part 1, model based on tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f4760a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 628,  484,  186],\n",
       "       [ 653, 1314,  196],\n",
       "       [ 612,  451,  543]], dtype=int64)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cf_mat1=confusion_matrix(rtfidf, ptfidf, labels=[0, 1, 2])\n",
    "cf_mat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ab9e93f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(cf_mat1, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e45deb",
   "metadata": {},
   "source": [
    "#### part 2, model based on svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4e273908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[639, 227, 358],\n",
       "       [667, 837, 572],\n",
       "       [487, 258, 824]], dtype=int64)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_mat2=confusion_matrix(rsvm, psvm, labels=[0, 1, 2])\n",
    "cf_mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ff8dd10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cf_mat2, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6114d62",
   "metadata": {},
   "source": [
    "#### part 3, model based on GloVe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6b844291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 838,  385,   75],\n",
       "       [ 811, 1181,  171],\n",
       "       [ 714,  500,  392]], dtype=int64)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_mat3=confusion_matrix(torch.tensor(rglove).to(\"cpu\"), torch.tensor(pglove).to(\"cpu\"), labels=[0, 1, 2])\n",
    "cf_mat3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ead5c303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cf_mat3, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0e378b",
   "metadata": {},
   "source": [
    "#### part 4, model based on Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "bc91a12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 809,  287,  202],\n",
       "       [ 525, 1419,  219],\n",
       "       [ 489,  310,  807]], dtype=int64)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_mat4=confusion_matrix(rbert, pbert, labels=[0, 1, 2])\n",
    "cf_mat4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4ea597c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cf_mat4, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852c8643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
